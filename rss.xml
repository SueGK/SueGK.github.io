<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://suegk.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>Digest Today</title>
    <link>https://suegk.github.io/</link>
    <description>Artificial Intelligence | Deep Learning | Productivity Software | Note-Taking</description>
    <language>en-US</language>
    <pubDate>Wed, 28 Aug 2024 04:57:20 GMT</pubDate>
    <lastBuildDate>Wed, 28 Aug 2024 04:57:20 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <category>æ•ˆç‡å·¥å…·</category>
    <category>ML</category>
    <category>å­¦ä¹ èµ„æº</category>
    <category>Pytorch</category>
    <category>CV</category>
    <item>
      <title>æ•ˆç‡å·¥å…·ï¼šRoam Researchä¸­Discourse Graphæ’ä»¶ä½¿ç”¨æµç¨‹</title>
      <link>https://suegk.github.io/posts/2022-06-30.html</link>
      <guid>https://suegk.github.io/posts/2022-06-30.html</guid>
      <source url="https://suegk.github.io/rss.xml">æ•ˆç‡å·¥å…·ï¼šRoam Researchä¸­Discourse Graphæ’ä»¶ä½¿ç”¨æµç¨‹</source>
      <description>Iâ€™m still surprised how much Roam Research potential would you find! Have been trying to use roam for my machine learning study for 2 weeks. The combination of discourse graph and query builder in @roam_js is really powerful! Here are what I found useful:æˆ‘ä»ç„¶å¾ˆæƒŠè®¶ä½ ä¼šå‘ç° Roam Research çš„æ½œåŠ›æœ‰å¤šå¤§ï¼2 å‘¨ä»¥æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨å°è¯•ä½¿ç”¨ roam è¿›è¡Œæˆ‘çš„æœºå™¨å­¦ä¹ å­¦ä¹ ã€‚@roam_js&amp;nbsp;ä¸­ discourse graph å’Œ query builder çš„ç»„åˆçœŸçš„å¾ˆå¼ºå¤§ï¼ä»¥ä¸‹æ˜¯æˆ‘å‘ç°æœ‰ç”¨çš„å†…å®¹ï¼š</description>
      <category>æ•ˆç‡å·¥å…·</category>
      <pubDate>Thu, 30 Jun 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>Iâ€™m still surprised how much Roam Research potential would you find! Have been trying to use roam for my machine learning study for 2 weeks. The combination of discourse graph and query builder in <a href="https://twitter.com/roam_js" target="_blank" rel="noopener noreferrer">@roam_js</a> is really powerful! Here are what I found useful:æˆ‘ä»ç„¶å¾ˆæƒŠè®¶ä½ ä¼šå‘ç° Roam Research çš„æ½œåŠ›æœ‰å¤šå¤§ï¼2 å‘¨ä»¥æ¥ï¼Œæˆ‘ä¸€ç›´åœ¨å°è¯•ä½¿ç”¨ roam è¿›è¡Œæˆ‘çš„æœºå™¨å­¦ä¹ å­¦ä¹ ã€‚@roam_js&nbsp;ä¸­ discourse graph å’Œ query builder çš„ç»„åˆçœŸçš„å¾ˆå¼ºå¤§ï¼ä»¥ä¸‹æ˜¯æˆ‘å‘ç°æœ‰ç”¨çš„å†…å®¹ï¼š</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*GHCaGMokzGHolVp-.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*KwhYhr0UaYP9XO16.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><a href="https://medium.com/@sue.sk.guo/((WjDE4sICN))" target="_blank" rel="noopener noreferrer">*</a> is the alias for the code node.*ï¼‰æ˜¯ Code èŠ‚ç‚¹çš„åˆ«åã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:318/0*32JqBwp8RBRywxHu.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The reason why I add both code node and alias for that code node is that Discourse canâ€™t recognize a block reference of a code node as code node to establish the relationship. So like the gif below, no discourse relation found.æˆ‘ä¹‹æ‰€ä»¥åŒæ—¶ä¸ºè¯¥ä»£ç èŠ‚ç‚¹æ·»åŠ  code node å’Œ aliasï¼Œæ˜¯å› ä¸º Discourse æ— æ³•å°†ä»£ç èŠ‚ç‚¹çš„å—å¼•ç”¨è¯†åˆ«ä¸ºä»£ç èŠ‚ç‚¹æ¥å»ºç«‹å…³ç³»ã€‚æ‰€ä»¥å°±åƒä¸‹é¢çš„ gif ä¸€æ ·ï¼Œæ²¡æœ‰æ‰¾åˆ° discourse å…³ç³»ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*uJWjYEvnPgi5fohx.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*KpaOvhdCiabj25YY.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Now, I can easily go to the code node original block by clicking the * and then go back by checking the reference number.ç°åœ¨ï¼Œæˆ‘å¯ä»¥é€šè¿‡å•å‡» * è½»æ¾è½¬åˆ°ä»£ç èŠ‚ç‚¹åŸå§‹å—ï¼Œç„¶åé€šè¿‡æ£€æŸ¥å‚è€ƒç¼–å·è¿”å›ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*km3RxfLN8xIbtsh8.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/0*GHCaGMokzGHolVp-.png" type="image/png"/>
    </item>
    <item>
      <title>æ•ˆç‡å·¥å…·ï¼šRoam Researchä»£åŠä»»åŠ¡ç®¡ç†æµç¨‹</title>
      <link>https://suegk.github.io/posts/2022-07-08.html</link>
      <guid>https://suegk.github.io/posts/2022-07-08.html</guid>
      <source url="https://suegk.github.io/rss.xml">æ•ˆç‡å·¥å…·ï¼šRoam Researchä»£åŠä»»åŠ¡ç®¡ç†æµç¨‹</source>
      <description>For Blogs that need a lot of time to read and learnå¯¹äºéœ€è¦å¤§é‡æ—¶é—´é˜…è¯»å’Œå­¦ä¹ çš„åšå®¢ (such as code projects etc)ï¼ˆå¦‚ä»£ç é¡¹ç›®ç­‰ï¼‰ â–¶ Using Blog Reference like ((Blog)) to gather all blog/articles so I could easily check in the â€œReading&amp;Watching Listâ€ Pageâ–¶ ä½¿ç”¨åƒ&amp;nbsp;ï¼ˆï¼ˆBlogï¼‰ï¼‰&amp;nbsp;è¿™æ ·çš„åšå®¢å‚è€ƒæ¥æ”¶é›†æ‰€æœ‰åšå®¢/æ–‡ç« ï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥è½»æ¾æŸ¥çœ‹â€œé˜…è¯»&amp;è§‚çœ‹åˆ—è¡¨â€é¡µé¢</description>
      <category>æ•ˆç‡å·¥å…·</category>
      <pubDate>Fri, 08 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><strong>For Blogs that need a lot of time to read and learnå¯¹äºéœ€è¦å¤§é‡æ—¶é—´é˜…è¯»å’Œå­¦ä¹ çš„åšå®¢
(such as code projects etc)ï¼ˆå¦‚ä»£ç é¡¹ç›®ç­‰ï¼‰</strong></p>
<p>â–¶ Using Blog Reference like <strong>((Blog))</strong> to gather all blog/articles so I could easily check in the â€œReading&amp;Watching Listâ€ Pageâ–¶ ä½¿ç”¨åƒ&nbsp;ï¼ˆï¼ˆBlogï¼‰ï¼‰&nbsp;è¿™æ ·çš„åšå®¢å‚è€ƒæ¥æ”¶é›†æ‰€æœ‰åšå®¢/æ–‡ç« ï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥è½»æ¾æŸ¥çœ‹â€œé˜…è¯»&amp;è§‚çœ‹åˆ—è¡¨â€é¡µé¢</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*7NRdxjiFjzSzeAom" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*H1HNTw7kJAXXSNke" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><strong>For smaller todo list&nbsp;å¯¹äºè¾ƒå°çš„å¾…åŠäº‹é¡¹åˆ—è¡¨</strong></p>
<p>I just use roam default /todo(in any pages) and use roam/js smartblocks TODO commands to fetch a list of block references of TODOs for Today/Overdue/Future TODOSæˆ‘åªä½¿ç”¨ roam default /todoï¼ˆåœ¨ä»»ä½•é¡µé¢ä¸­ï¼‰å¹¶ä½¿ç”¨ roam/js smartblocks TODO å‘½ä»¤æ¥è·å–ä»Šå¤©/é€¾æœŸ/æœªæ¥ TODOS çš„ TODO çš„å—å¼•ç”¨åˆ—è¡¨</p>
<p>My Usage Scenarios: <strong>Morning Journal</strong>æˆ‘çš„ä½¿ç”¨åœºæ™¯ï¼šæ™¨æŠ¥</p>
<p>When I start to write Morning Journal, Iâ€™ll use #42SmartBlock to automatically create the journal with <strong>Get Thing Done</strong> section where it shows all the TODOs for Today/Overdue/Future TODOS. Therefore I could easily check up the random TODOs to find somethings youâ€™d like to do today and add them to <strong>Make time for todayâ€™s Highlight</strong> section. I also have projects pages where contains all todo for the specific project but sometimes itâ€™s convenient and mindless to just fetch some random todo blocks.å½“æˆ‘å¼€å§‹ç¼–å†™æ™¨é—´æ—¥è®°æ—¶ï¼Œæˆ‘å°†ä½¿ç”¨ #42SmartBlock è‡ªåŠ¨åˆ›å»ºå¸¦æœ‰&nbsp;Get Thing Done&nbsp;éƒ¨åˆ†çš„æ—¥è®°ï¼Œå…¶ä¸­æ˜¾ç¤ºä»Šå¤©/é€¾æœŸ/æœªæ¥ TODO çš„æ‰€æœ‰ TODOã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥è½»æ¾åœ°æ£€æŸ¥éšæœºçš„ TODOï¼Œæ‰¾åˆ°æ‚¨ä»Šå¤©æƒ³åšçš„äº‹æƒ…ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ°&nbsp;Make time for today's Highlight&nbsp;éƒ¨åˆ†ã€‚æˆ‘ä¹Ÿæœ‰é¡¹ç›®é¡µé¢ï¼Œå…¶ä¸­åŒ…å«ç‰¹å®šé¡¹ç›®çš„æ‰€æœ‰å¾…åŠäº‹é¡¹ï¼Œä½†æœ‰æ—¶åªæ˜¯è·å–ä¸€äº›éšæœºçš„å¾…åŠäº‹é¡¹å—æ—¢æ–¹ä¾¿åˆæ— æ„è¯†ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1064/1*xkkFs4Y29utCd7jBnYS0Cg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1136/1*9NOdgK41HM8KK2hbvZtECg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/0*7NRdxjiFjzSzeAom" type="image/"/>
    </item>
    <item>
      <title>MLå…¬å¼æ¨å¯¼ â€” Logistic Regression Gradient Descent Derivatives</title>
      <link>https://suegk.github.io/posts/2022-07-11.html</link>
      <guid>https://suegk.github.io/posts/2022-07-11.html</guid>
      <source url="https://suegk.github.io/rss.xml">MLå…¬å¼æ¨å¯¼ â€” Logistic Regression Gradient Descent Derivatives</source>
      <description>From Andrew Ng</description>
      <category>ML</category>
      <pubDate>Mon, 11 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<figure><img src="https://cdn-images-1.medium.com/max/3792/1*psolPGqHQtCuVdAsW0TE7A.png" alt="From Andrew Ng" tabindex="0" loading="lazy"><figcaption>From Andrew Ng</figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/3822/1*I-xYOKDargu0U75QkvoS_A.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/3792/1*psolPGqHQtCuVdAsW0TE7A.png" type="image/png"/>
    </item>
    <item>
      <title>æ•ˆç‡å·¥å…·ï¼šä¹¦ç®€ç®¡ç†å™¨Raindrop</title>
      <link>https://suegk.github.io/posts/2022-07-25.html</link>
      <guid>https://suegk.github.io/posts/2022-07-25.html</guid>
      <source url="https://suegk.github.io/rss.xml">æ•ˆç‡å·¥å…·ï¼šä¹¦ç®€ç®¡ç†å™¨Raindrop</source>
      <description>Tools:&amp;nbsp;å·¥å…·ï¼š @raindrop_io : bookmark manager/read later/highlighting@raindrop_io&amp;nbsp;ï¼š ä¹¦ç­¾ç®¡ç†å™¨/ç¨åé˜…è¯»/é«˜äº®æ˜¾ç¤º @IFTTT : automation&amp;workflow@IFTTT&amp;nbsp;ï¼š è‡ªåŠ¨åŒ–&amp;å·¥ä½œæµ You can customize the automation on ifttt. I normally choose the collection folder and tags.æ‚¨å¯ä»¥åœ¨ ifttt ä¸Šè‡ªå®šä¹‰è‡ªåŠ¨åŒ–ã€‚æˆ‘é€šå¸¸ä¼šé€‰æ‹©æ”¶è—æ–‡ä»¶å¤¹å’Œæ ‡ç­¾ã€‚</description>
      <category>æ•ˆç‡å·¥å…·</category>
      <pubDate>Mon, 25 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<h1> Tools:&nbsp;å·¥å…·ï¼š</h1>
<p><a href="http://twitter.com/raindrop_io" target="_blank" rel="noopener noreferrer">@raindrop_io</a> : bookmark manager/read later/highlighting@raindrop_io&nbsp;ï¼š ä¹¦ç­¾ç®¡ç†å™¨/ç¨åé˜…è¯»/é«˜äº®æ˜¾ç¤º
<a href="http://twitter.com/IFTTT" target="_blank" rel="noopener noreferrer">@IFTTT</a> : automation&amp;workflow@IFTTT&nbsp;ï¼š è‡ªåŠ¨åŒ–&amp;å·¥ä½œæµ
You can customize the automation on ifttt. I normally choose the collection folder and tags.æ‚¨å¯ä»¥åœ¨ ifttt ä¸Šè‡ªå®šä¹‰è‡ªåŠ¨åŒ–ã€‚æˆ‘é€šå¸¸ä¼šé€‰æ‹©æ”¶è—æ–‡ä»¶å¤¹å’Œæ ‡ç­¾ã€‚</p>
<p>Raindrop is a bookmark manager which is able to collect everything you come across while browsing such as articles, tweets, YouTube videos and so on.Raindrop æ˜¯ä¸€ä¸ªä¹¦ç­¾ç®¡ç†å™¨ï¼Œå®ƒèƒ½å¤Ÿæ”¶é›†æ‚¨åœ¨æµè§ˆæ—¶é‡åˆ°çš„æ‰€æœ‰å†…å®¹ï¼Œä¾‹å¦‚æ–‡ç« ã€æ¨æ–‡ã€YouTube è§†é¢‘ç­‰ã€‚</p>
<h1> Supported File Typesæ”¯æŒçš„æ–‡ä»¶ç±»å‹</h1>
<ul>
<li>Images (<code>jpeg</code>, <code>gif</code>, <code>png</code>)å›¾ç‰‡ï¼ˆjpegã€gifã€pngï¼‰</li>
<li>Videos (<code>mp4</code>, <code>mov</code>, <code>wmv</code>, <code>webm</code>)è§†é¢‘ï¼ˆmp4ã€movã€wmvã€webmï¼‰</li>
<li>Documents (<code>pdf</code>, <code>doc</code>, <code>docx</code>, <code>xls</code>, <code>xlsx</code>, <code>txt</code>)æ–‡æ¡£ ï¼ˆpdfï¼Œ&nbsp;docï¼Œ docxï¼Œ&nbsp;xlsï¼Œ xlsxï¼Œ&nbsp;txtï¼‰</li>
</ul>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*TMjVWkF1M66mQot_o9SNFQ.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>from <a href="http://Raindrop.io" target="_blank" rel="noopener noreferrer">Raindrop.io</a> website&nbsp;ä» <a href="http://Raindrop.io" target="_blank" rel="noopener noreferrer">Raindrop.io</a> ç½‘ç«™</p>
<p>It has many powerful features.å®ƒæœ‰è®¸å¤šå¼ºå¤§çš„åŠŸèƒ½ã€‚</p>
<h1> 1. tags&amp;filters&nbsp;1. æ ‡ç­¾å’Œè¿‡æ»¤å™¨</h1>
<p>You can search your collected website by tags or filters.æ‚¨å¯ä»¥æŒ‰æ ‡ç­¾æˆ–è¿‡æ»¤å™¨æœç´¢æ‚¨æ”¶é›†çš„ç½‘ç«™ã€‚
You can tag items and filter items with specific tags in specific collection folders.æ‚¨å¯ä»¥åœ¨ç‰¹å®šé›†åˆæ–‡ä»¶å¤¹ä¸­ä½¿ç”¨ç‰¹å®šæ ‡ç­¾æ ‡è®°é¡¹ç›®å’Œç­›é€‰é¡¹ç›®ã€‚
Filters automatically gather different types of bookmarks including Highights/Links/Articles/Video/Documents/Without lags.è¿‡æ»¤å™¨ä¼šè‡ªåŠ¨æ”¶é›†ä¸åŒç±»å‹çš„ä¹¦ç­¾ï¼ŒåŒ…æ‹¬ Highights/Links/Articles/Video/Documents/Without lagsã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1398/1*iD5GOFP39K8WrIkCxr7XEw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1656/1*zm0vC8DtiyG-TGdI-VgFBw.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h1> 2. Integrations â€” Automation2. é›†æˆ â€” è‡ªåŠ¨åŒ–</h1>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*oQqf9OV6NQdBhAbvCn3UrA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1276/1*30a9Fi13TmR0-zQD3QeTlw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>from <a href="http://raindrop.io" target="_blank" rel="noopener noreferrer">raindrop.io</a> website&nbsp;ä» <a href="http://raindrop.io" target="_blank" rel="noopener noreferrer">raindrop.io</a> ç½‘ç«™</p>
<p>You can customize the automation on ifttt. I normally choose the collection folder and tags. You could automately collect liked tweets, liked YouTube videos and Medium Bookmarks into Raindrop.æ‚¨å¯ä»¥åœ¨ ifttt ä¸Šè‡ªå®šä¹‰è‡ªåŠ¨åŒ–ã€‚æˆ‘é€šå¸¸ä¼šé€‰æ‹©æ”¶è—æ–‡ä»¶å¤¹å’Œæ ‡ç­¾ã€‚æ‚¨å¯ä»¥è‡ªåŠ¨å°†å–œæ¬¢çš„æ¨æ–‡ã€å–œæ¬¢çš„ YouTube è§†é¢‘å’Œ Medium ä¹¦ç­¾æ”¶é›†åˆ° Raindrop ä¸­ã€‚</p>
<p>Here are my IFTTT settings:ä»¥ä¸‹æ˜¯æˆ‘çš„ IFTTT è®¾ç½®ï¼š</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:514/0*yzcDMFEpKdMIIeYK.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Create Twitter/Medium folders
åˆ›å»º Twitter/Medium æ–‡ä»¶å¤¹</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:332/0*recwsFiMLJUOjKCC.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>customized in IFTTT&nbsp;åœ¨ IFTTT ä¸­å®šåˆ¶</p>
<ol start="3">
<li>Highlighting&nbsp;3. çªå‡ºæ˜¾ç¤º</li>
</ol>
<p>You can highlight the articles you saved in raindrop and export the highlights.æ‚¨å¯ä»¥çªå‡ºæ˜¾ç¤ºä¿å­˜åœ¨ raindrop ä¸­çš„æ–‡ç« å¹¶å¯¼å‡ºçªå‡ºæ˜¾ç¤ºã€‚</p>
<p>The IMPORTANT THING IS!!!é‡è¦çš„æ˜¯!!</p>
<blockquote>
<p>Thereâ€™s no limit on total number of pages and highlights.é¡µé¢æ€»æ•°å’Œ highlights æ€»æ•°æ²¡æœ‰é™åˆ¶ã€‚</p>
</blockquote>
<p>There are still more powerful features you could explore in their website!æ‚¨è¿˜å¯ä»¥åœ¨ä»–ä»¬çš„ç½‘ç«™ä¸­æ¢ç´¢æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*Otyg6Anyd4qtgV3f-BwTWA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/1*TMjVWkF1M66mQot_o9SNFQ.gif" type="image/gif"/>
    </item>
    <item>
      <title>å¦‚ä½•æ”¹å˜GitHubä¸­ç…§ç‰‡çš„ä½ç½®</title>
      <link>https://suegk.github.io/posts/2022-08-16.html</link>
      <guid>https://suegk.github.io/posts/2022-08-16.html</guid>
      <source url="https://suegk.github.io/rss.xml">å¦‚ä½•æ”¹å˜GitHubä¸­ç…§ç‰‡çš„ä½ç½®</source>
      <description>You need to use some HTML in your Markdown. You could also use some text expander app to type it more quickly. If you prefer video, here is my Youtube video! Video Link Here is my Macro for Keyboard Maestro which automate applications or web sites, text or images, simple or complex, on command or scheduled. You can also use other software such as TextExpander, ProKeys chrome extension, Alfred Snippets etc.</description>
      <category>å­¦ä¹ èµ„æº</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>You need to use some HTML in your Markdown. You could also use some text expander app to type it more quickly.</p>
<p>If you prefer video, here is my Youtube video! <a href="https://www.youtube.com/watch?v=8LKybiFA-is" target="_blank" rel="noopener noreferrer">Video Link</a></p>
<p>Here is my Macro for <a href="https://www.keyboardmaestro.com/main/" target="_blank" rel="noopener noreferrer">Keyboard Maestro</a> which automate applications or web sites, text or images, simple or complex, on command or scheduled. You can also use other software such as TextExpander, ProKeys chrome extension, Alfred Snippets etc.</p>
<p>Please download it here: <a href="https://forum.keyboardmaestro.com/t/resize-image-and-change-location-in-github/28632" target="_blank" rel="noopener noreferrer">link</a></p>
<ul>
<li>Resize pictures</li>
<li>Change location of pictures</li>
<li>Add many pictures in one row</li>
</ul>
<h2> Resize pictures</h2>
<p>You can custimize the width and height.</p>
<pre><code>`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;img src="yourPictureURL.jpg" width="400" height="200"/&gt;`
</code></pre>
<p>Before: Markdown</p>
<pre><code>![]([https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png](https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png))
</code></pre>
<p>After: HTML</p>
<pre><code>&lt;img src=â€[https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png](https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png)" width=â€400"&gt;
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*4G0C2BUOOMRO_CayzpqjvA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Change location of resized pictures</h2>
<p>Put the picture on the right:</p>
<pre><code>`&lt;div align="right"&gt;`
`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;/div&gt;`
</code></pre>
<p>Put the picture on the center:</p>
<pre><code>`&lt;div align="center"&gt;`
`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;/div&gt;`
</code></pre>
<p>If you donâ€™t want to resize pictures then you can remove width="400" .</p>
<figure><img src="https://cdn-images-1.medium.com/max/2076/1*4Fo9_Js2ZV91cWDCqrOsaw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Add many pictures in one row</h2>
<pre><code>`&lt;p float="left"&gt;`
 `&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
 `&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
 .....
`&lt;/p&gt;`
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*_DI25TlR2A3aLOb4O5UpEw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Macro Download</h2>
<p>I made some Macro to easily type the HTML code.</p>
<p><strong>First, get the image url.</strong>
Iâ€™m using an file hosting tool called <a href="https://apps.apple.com/us/app/upic-hosting-tool/id1510718678" target="_blank" rel="noopener noreferrer">uPic</a> to get image url. You can also upload images in your GitHub to get image url.</p>
<p><strong>Second, Typed shortcuts are:</strong></p>
<pre><code>?zz  # image resize
?zc  # image resize and center location
?zr  # image resize and right location
?zm  # multiple image in one row
</code></pre>
<p>You can also change to hot key shortcuts or just call macro directly.</p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*V4MmQDaXqW6wWI3mcRvx8w.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/3160/1*WuFAjNBSaPeSNScNInF-hg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*fvGFEbZHbIZpi5qoDsrDAA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
    </item>
    <item>
      <title>å‘ç°çƒ­é—¨ ML/DL ç ”ç©¶è®ºæ–‡çš„ç½‘ç«™</title>
      <link>https://suegk.github.io/posts/2022-08-18-1.html</link>
      <guid>https://suegk.github.io/posts/2022-08-18-1.html</guid>
      <source url="https://suegk.github.io/rss.xml">å‘ç°çƒ­é—¨ ML/DL ç ”ç©¶è®ºæ–‡çš„ç½‘ç«™</source>
      <description>Keep you updated with state of the art technology is really important in the Machine learning and deep learning field. è®©æ‚¨äº†è§£æœ€å…ˆè¿›çš„æŠ€æœ¯åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸéå¸¸é‡è¦ã€‚ There are some tips from Andre NG on how to read research papers. https://www.youtube.com/watch?v=733m6qBH-jI</description>
      <category>å­¦ä¹ èµ„æº</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>Keep you updated with state of the art technology is really important in the Machine learning and deep learning field.</p>
<p>è®©æ‚¨äº†è§£æœ€å…ˆè¿›çš„æŠ€æœ¯åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸéå¸¸é‡è¦ã€‚</p>
<p>There are some tips from Andre NG on how to read research papers. <a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>Andre NG æä¾›äº†ä¸€äº›å…³äºå¦‚ä½•é˜…è¯»ç ”ç©¶è®ºæ–‡çš„å»ºè®®ã€‚</p>
<p><a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>So, where can we find the popular papers?é‚£ä¹ˆï¼Œæˆ‘ä»¬åœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°çƒ­é—¨è®ºæ–‡å‘¢ï¼Ÿ=</p>
<h2> * paperswithcode â€” <a href="https://paperswithcode.com/sota" target="_blank" rel="noopener noreferrer">Browse State-of-the-Art</a></h2>
<p>The papers are well categorized so you can follow what =Andre =said, choose an area of interest like semantic segmentation, and read 15â€“20 papers to get a good understanding of this field. More importantly, you can find the paperâ€™s code.è¿™äº›è®ºæ–‡åˆ†ç±»å¾ˆå¥½ï¼Œå› æ­¤æ‚¨å¯ä»¥æŒ‰ç…§&nbsp;Andre&nbsp;æ‰€è¯´çš„å†…å®¹ï¼Œé€‰æ‹©ä¸€ä¸ªæ„Ÿå…´è¶£çš„é¢†åŸŸï¼Œä¾‹å¦‚è¯­ä¹‰åˆ†å‰²ï¼Œå¹¶é˜…è¯» 15-20 ç¯‡è®ºæ–‡ä»¥å¾ˆå¥½åœ°ç†è§£è¯¥é¢†åŸŸã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°è®ºæ–‡çš„ä»£ç ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1048/1*GFD5JPhEEXzp7vwMmKfpjw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> * <a href="https://nn.labml.ai/index.html" target="_blank" rel="noopener noreferrer">labml.ai Deep Learning Paper Implementations</a></h2>
<p>59 Implementations/tutorials of deep learning papers with side-by-side notes ğŸ“; including transformers (original, xl, switch, feedback, vit, â€¦), optimizers (adam, adabelief, â€¦), gans(cyclegan, stylegan2, â€¦), ğŸ® reinforcement learning (ppo, dqn), capsnet, distillation, â€¦ ğŸ§ 59 æ·±åº¦å­¦ä¹ è®ºæ–‡çš„å®ç°/æ•™ç¨‹ï¼Œå¹¶æ’æ³¨é‡ŠğŸ“;åŒ…æ‹¬ Transformer ï¼ˆOriginalï¼Œ XLï¼Œ Switchï¼Œ Feedbackï¼Œ VITç­‰ï¼‰ï¼Œ ä¼˜åŒ–å™¨ ï¼ˆAdamï¼Œ AdaBeliefï¼Œ ...ï¼‰ï¼Œ GANSï¼ˆCycleganï¼Œ StyleGan2ï¼Œ ...ï¼‰ï¼Œ ğŸ® å¼ºåŒ–å­¦ä¹  ï¼ˆPPOï¼Œ DQNï¼‰ï¼Œ CapsNetï¼Œ Distillation ...ğŸ§ </p>
<p>This is a collection of simple PyTorch implementations of neural networks and related algorithms.  These implementations are documented with explanations. So you are able to read the paper while understanding how to implement it by Pytorch.è¿™æ˜¯ç¥ç»ç½‘ç»œå’Œç›¸å…³ç®—æ³•çš„ç®€å• PyTorch å®ç°çš„é›†åˆã€‚<mark>è¿™äº›å®ç°è®°å½•äº†è¯´æ˜ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥åœ¨é˜…è¯»æœ¬æ–‡çš„åŒæ—¶äº†è§£å¦‚ä½•é€šè¿‡ Pytorch å®ç°å®ƒã€‚</mark></p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*QI_S19lNihupZgpJj7eMfg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> * <a href="http://labml.ai" target="_blank" rel="noopener noreferrer">labml.ai</a> â€” <a href="https://papers.labml.ai/" target="_blank" rel="noopener noreferrer">Trending Research Papers</a></h2>
<p>The most popular research papers on social media like Twitter. You can easily find links to download papers, paper summaries, explanation videos, and discussions.Twitter ç­‰ç¤¾äº¤åª’ä½“ä¸Šæœ€å—æ¬¢è¿çš„ç ”ç©¶è®ºæ–‡ã€‚æ‚¨å¯ä»¥è½»æ¾æ‰¾åˆ°ä¸‹è½½è®ºæ–‡ã€è®ºæ–‡æ‘˜è¦ã€è§£é‡Šè§†é¢‘å’Œè®¨è®ºçš„é“¾æ¥ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*7pr_Bk_Sh38aNgmzOPgb1A.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*XJkaJCU4qvndNY1pUV5VZw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*MxxeMhbirfw3CcN9Vs4eRg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The chrome extension is really helpful as well.chrome æ‰©å±•ç¨‹åºä¹Ÿéå¸¸æœ‰ç”¨ã€‚</p>
<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>This extension shows you the following details about research papers:
âœ¨ 2-line summary
âœ¨ Availability source code, videos, and discussions
âœ¨ Popularity on Twitter
âœ¨ Conferences
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> * <a href="https://aman.ai/papers/#noise-contrastive-estimation-a-new-estimation-principle-for-unnormalized-statistical-models" target="_blank" rel="noopener noreferrer">Paper lists made by Aman</a></h2>
<p>A summary of key papers in Computer Vision, NLP, and Speech recognition.</p>
<p>è®¡ç®—æœºè§†è§‰ã€NLP å’Œè¯­éŸ³è¯†åˆ«æ–¹é¢çš„å…³é”®è®ºæ–‡æ‘˜è¦ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*BdkxFnegXS0g0sHUNW-Akw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="https://deeplearn.org/" target="_blank" rel="noopener noreferrer">Deep Learning Monitor</a></h2>
<p>=Another website where you can find the hot papers on social media.å¦ä¸€ä¸ªæ‚¨å¯ä»¥åœ¨ç¤¾äº¤åª’ä½“ä¸Šæ‰¾åˆ°çƒ­é—¨è®ºæ–‡çš„ç½‘ç«™ã€‚=</p>
<p>The great feature is that you can create some monitors with the keywords related to the topic of interest and check new updates every one or two weeks. Once you find a good paper and you log in Mendeley on this website, you can directly send it to your Mendeley account.å¾ˆæ£’çš„åŠŸèƒ½æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨ä¸æ„Ÿå…´è¶£ä¸»é¢˜ç›¸å…³çš„å…³é”®å­—åˆ›å»ºä¸€äº›ç›‘è§†å™¨ï¼Œå¹¶æ¯éš”ä¸€ä¸¤å‘¨æ£€æŸ¥ä¸€æ¬¡æ–°çš„æ›´æ–°ã€‚ä¸€æ—¦ä½ æ‰¾åˆ°ä¸€ç¯‡å¥½çš„è®ºæ–‡å¹¶åœ¨æœ¬ç½‘ç«™ä¸Šç™»å½• Mendeleyï¼Œä½ å°±å¯ä»¥ç›´æ¥å°†å…¶å‘é€åˆ°ä½ çš„ Mendeley å¸æˆ·ã€‚</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*3HtQpm1hSI6ApIAIOl3YvQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*cz1BAvWn42uztuayw0dm-w.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" type="image/png"/>
    </item>
    <item>
      <title>OpenCV-color in BGR order you must know</title>
      <link>https://suegk.github.io/posts/2022-08-18.html</link>
      <guid>https://suegk.github.io/posts/2022-08-18.html</guid>
      <source url="https://suegk.github.io/rss.xml">OpenCV-color in BGR order you must know</source>
      <description>Wanna play around with the code? Link import os import numpy as np import argparse import cv2 import matplotlib.pyplot as plt</description>
      <category>å­¦ä¹ èµ„æº</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>Wanna play around with the code? <a href="https://github.com/SueGK/Courses/blob/main/pyimagesearch-opencv-17-day-course/OpenCV-Mynotes/opencv_BGR_color.ipynb" target="_blank" rel="noopener noreferrer">Link</a></p>
<pre><code>import os
import numpy as np
import argparse
import cv2
import matplotlib.pyplot as plt
</code></pre>
<p>The original picture looks like:</p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/0*gMD-zYzvJWZegLIH.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The OpenCV assumes images are in BGR channel order. OpenCV imread, imwrite and imshow all work with the BGR order, so the image won't change if we use cv2.imshow to show the image. But it doesn't work with matplotlib.</p>
<p>Most image processing library use the RGB ordering such as matplotlib so if use plt.imshow, the color of the logo changed.</p>
<pre><code>img = cv2.imread("logo.png")

# show the image by cv2
# The cv2.imshow() and cv.imshow() functions from the opencv-python package are incompatible with Jupyter notebook;
# see https://github.com/jupyter/notebook/issues/3935.
# As a replacement, you can use the following function:
from google.colab.patches import cv2_imshow
cv2_imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*Ixo3Bu6gZwaoAGhT64Rlyg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code># show the image by matplotlib
plt.subplot(111)
plt.imshow(img)
plt.title("Original")
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*sqd7CCalX7L04M21PYdCvA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>If we want to read image in RGB order in OpenCV, we can use:</p>
<p>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</p>
<pre><code># read images in RGB order in OpenCV
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
from google.colab.patches import cv2_imshow
cv2_imshow(img_rgb)
plt.imshow(img_rgb)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*b6K55FOZwp3tePqYe-DERg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><strong>cv2.imread(FILENAME, FLAG)</strong></p>
<p>There are some flags:</p>
<p><strong>cv2.IMREAD_UNCHANGED:</strong> read the image as is from the source(with alpha channel). If the source image is an RGB, it loads the image into array with Red, Green and Blue channels.</p>
<p><strong>cv2.IMREAD_COLOR:</strong> convert image to the 3 channel BGR color image but no transparency channel</p>
<p><strong>cv2.IMREAD_GRAYSCALE:</strong> convert image to the single channel grayscale image</p>
<p>Check more flags here: <a href="https://docs.opencv.org/3.4/d8/d6a/group__imgcodecs__flags.html#ga61d9b0126a3e57d9277ac48327799c80" target="_blank" rel="noopener noreferrer">OpenCV: Flags used for image file reading and writing</a></p>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_UNCHANGED)
cv2_imshow(img)
plt.imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*d_zvffWdR2cnGDKDOuG0DA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_COLOR)
cv2_imshow(img)
plt.imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*bz2dYJPqveXPof4lw4f24A.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_GRAYSCALE)
cv2_imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*yYNiNY657JdYVMZMLnqlVg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Let's have a look at the BGR channel. If we want to print the whole array then set np.set_printoptions(threshold=np.inf) and set it back to default after print the array. np.set_printoptions(threshold=1000</p>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_COLOR)
cv2_imshow(img)
b = img[:,:,0] # get blue channel
g = img[:,:,1] # get green channel
r = img[:,:,2] # get red channel

print(b)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*aLUmFMIu3E8WJ6zMM0_t8g.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>np.set_printoptions(threshold=np.inf)
print(b)
np.set_printoptions(threshold=1000)

# the output is really long so I won't print it here.
</code></pre>
<p>Now let's convert it to <strong>RGB</strong> channel. Even though we read image in RGB order but the logo color will change if we use cv2.imshow to show the image because it works with <strong>BGR</strong> order. plt.imshow show the original color of the image because it works with <strong>RGB</strong> order as well.</p>
<pre><code># read images in RGB order in OpenCV
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
from google.colab.patches import cv2_imshow
cv2_imshow(img_rgb)
plt.imshow(img_rgb)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*abBgvTaTAg_wKCYiWBMuPQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>r1 = img_rgb[:,:,0] # get blue channel
g1 = img_rgb[:,:,1] # get green channel
b1 = img_rgb[:,:,2] # get red channel

(img[:,:,0] == img_rgb[:,:,2]).all()

Output: True
</code></pre>
<p>When you try to draw a rectangle box in OpenCV, be careful that the color of rectangle is also in <strong>BGR</strong> order.</p>
<pre><code># read the image
image = cv2.imread('logo.png')

# represents the top left corner of rectangle
start_point = (5, 5)

# represents the bottom right corner of rectangle
end_point = (20, 20)

# choose the rectangle color in BGR
color = (0, 0, 255) # red

# thickness of lines that make up the rectangle is 2 px
thickness = 2

# draw a rectangle with red line borders of thickness of 2 px
image = cv2.rectangle(image, start_point, end_point, color, thickness)

# Displaying the image
cv2_imshow(image)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*J3rSiqjnPRM1D04oPitI1Q.png" alt="" loading="lazy">
<a href="https://medium.com/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb" target="_blank" rel="noopener noreferrer"><strong>Mlearning.ai Submission Suggestions</strong>
<em>How to become a writer on Mlearning.ai</em>medium.com</a></p>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/2000/0*gMD-zYzvJWZegLIH.png" type="image/png"/>
    </item>
    <item>
      <title>Pytorch DataLoaderä½¿ç”¨æŒ‡å—</title>
      <link>https://suegk.github.io/posts/2022-12-06.html</link>
      <guid>https://suegk.github.io/posts/2022-12-06.html</guid>
      <source url="https://suegk.github.io/rss.xml">Pytorch DataLoaderä½¿ç”¨æŒ‡å—</source>
      <description>by Sue Create a custom dataset To create a customized dataset in PyTorch, you should create a subclass of the torch.utils.data.Dataset class and implement the following methods:</description>
      <category>Pytorch</category>
      <pubDate>Tue, 06 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<figure><img src="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" alt="by Sue" tabindex="0" loading="lazy"><figcaption>by Sue</figcaption></figure>
<h2> <strong>Create a custom dataset</strong></h2>
<p>To create a customized dataset in PyTorch, you should create a subclass of the torch.utils.data.Dataset class and implement the following methods:</p>
<ul>
<li><strong>init</strong>(): This method should initialize the dataset and any instance variables that you need. It should accept any arguments necessary to create the dataset.</li>
<li><strong>len</strong>(): This method should return the length of the dataset (i.e. the number of samples in the dataset).</li>
<li><strong>getitem</strong>(idx): This method should return the data for the sample with the specified index. The returned data should be a tuple containing the data and the corresponding label for the sample.</li>
</ul>
<p>Here is an example of how your custom dataset class might look:</p>
<pre><code>import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def__init__(self, data, labels):
        self.data = data
        self.labels = labels

def__len__(self):
        return len(self.labels)

def__getitem__(self, idx):
        return self.data[idx], self.labels[idx]
</code></pre>
<p>To use your custom dataset, you can create an instance of the class and then pass it to a PyTorch DataLoader object. The DataLoader can then be used to iterate over the dataset and retrieve the samples.</p>
<pre><code># Create an instance of the CustomDataset class
dataset = CustomDataset(data, labels)

# Create a DataLoader for the dataset
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

# Loop over the dataloader to retrieve the samples
for data, labels in dataloader:
    # Do something with the data and labels
</code></pre>
<p>You can learn more about creating custom datasets in PyTorch in the official PyTorch documentation: <a href="https://pytorch.org/docs/stable/data.html#creating-custom-datasets" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/data.html#creating-custom-datasets</a></p>
<h2> <strong>what does dataloader do?</strong></h2>
<p>A PyTorch DataLoader is an object that provides a number of benefits when working with large datasets. It is typically used in conjunction with a Dataset object that provides the data that the DataLoader will iterate over.</p>
<p>The main purpose of the DataLoader is to batch the data from the Dataset and provide it to the model during training. It allows you to specify the batch size and whether or not the data should be shuffled each epoch. This can be useful for training models on large datasets that don't fit in memory, as it allows the data to be processed in smaller batches.</p>
<p>In addition to batching the data, the DataLoader can also be used to perform data augmentation and preprocessing on the fly. This can be useful for tasks like image classification, where you may want to apply random transformations to the images in each batch to improve the model's generalization ability.</p>
<p>Overall, the DataLoader is an important part of the PyTorch data processing pipeline, and is typically used when training deep learning models on large datasets. It provides an iterator over a dataset, allowing you to train your model on a large dataset by loading only a small portion of the data into memory at a time.</p>
<p>The DataLoader class takes a dataset and a batch size as input and returns an iterator over the dataset that yields mini-batches of data. You can use the DataLoader class to shuffle the data and define the number of workers that will be used to load the data in parallel.</p>
<p>One way to use the DataLoader class is with the for loop. Here is an example:</p>
<pre><code># Import the DataLoader class
from torch.utils.data import DataLoader

# Create a dataset
dataset = SomeDataset()

# Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Use the DataLoader object like an iterator
for data in data_loader:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<p>The DataLoader class also has a <strong>iter</strong> method that returns an iterator over the dataset, allowing you to use the DataLoader instance in a for loop.</p>
<pre><code># Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Get an iterator over the dataset
data_iterator = iter(data_loader)

# Use the iterator in a for loop
for data in data_iterator:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<h2> <strong>collate_fn in DataLoader</strong></h2>
<p>The DataLoader class also provides a way to customize the way data is loaded by defining a collate_fn function. The collate_fn function defines how the data will be combined into a mini-batch. This is useful when your dataset contains data of different sizes, such as images of different sizes.</p>
<p>Here is an example of using a collate_fn function:</p>
<pre><code># Import the DataLoader class
from torch.utils.data import DataLoader

# Create a dataset
dataset = SomeDataset()

# Define a collate function
def collate_fn(data):
    # Sort the data in descending order of length
    data.sort(key=lambda x: len(x[0]), reverse=True)

# Unpack the data
    inputs, labels = zip(*data)

# Pad the inputs
    inputs = pad_sequence(inputs, batch_first=True)

return inputs, labels

# Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)

# Use the DataLoader object like an iterator
for data in data_loader:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<p><strong>Flowchat: How does DataLoader process data?</strong></p>
<figure><img src="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" alt="by Sue" tabindex="0" loading="lazy"><figcaption>by Sue</figcaption></figure>
<pre><code>num_dataset = 160
batch_size = 16
iteration = num_dataset / batch_size = 10 for i, data in enumerate(train_loader):
inputs, labels = data
</code></pre>
<p>When using a DataLoader instance in PyTorch, you can iterate over it in a for loop to retrieve the data in mini-batches.</p>
<p>In this example, the DataLoader instance will use a DataLoaderIter instance to load the data. The DataLoaderIter instance will call the Sampler to generate a list of indices that specify which elements from the dataset should be included in the mini-batch. The DataLoaderIter will then use a DatasetFetcher instance to retrieve the data from the dataset using the generated indices.</p>
<p>The DatasetFetcher instance will call the <strong>getitem</strong> method of the Dataset to retrieve the data for each index in the list of indices. This will return a list of data samples, where each sample is a tuple containing the input data and the corresponding label.</p>
<p>Finally, the DataLoaderIter will use the collate_fn function to combine the data samples into a mini-batch. The collate_fn function can be customized to define how the data will be combined into a mini-batch. The output of the collate_fn function is a mini-batch of data that is ready to be used by your model for training or evaluation.</p>
<p>In summary, when using a DataLoader instance in PyTorch,</p>
<ul>
<li>The resulting mini-batch of data is then yielded by the DataLoader instance in a for loop, and then decide whether to use a single or multi-process DataLoaderIter depending on whether multi-processing is used.</li>
<li>the data is loaded in mini-batches using a Sampler to generate indices</li>
<li>a DatasetFetcher to retrieve the data from the dataset based on the indices. In the DatasetFetcher, the <strong>getitem</strong>() method of the Dataset is called to get the real data. The data obtained here is a list, where each element is a tuple of (img, label)</li>
<li>a collate_fn function to combine the data into a mini-batch. So the data here is a list containing two elements, the tenser of img and label respectively.</li>
</ul>
<p>Thank you for reading this post. If you enjoyed it, please consider following me on Medium and <a href="https://twitter.com/Sue_sk79" target="_blank" rel="noopener noreferrer">twitter</a> for more content about productivity tools and AI! ğŸ”¥</p>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" type="image/"/>
    </item>
    <item>
      <title>All you should know about translation equivariance/invariance in CNN</title>
      <link>https://suegk.github.io/posts/2023-03-25.html</link>
      <guid>https://suegk.github.io/posts/2023-03-25.html</guid>
      <source url="https://suegk.github.io/rss.xml">All you should know about translation equivariance/invariance in CNN</source>
      <description>All you should know about translation equivariance/invariance in CNN First published on Medium Translation invariance and translation equivariance are two important concepts in convolutional neural networks (CNNs) related to the networkâ€™s ability to recognise objects regardless of their position within an image.</description>
      <category>CV</category>
      <pubDate>Sat, 25 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<h2> All you should know about translation equivariance/invariance in CNN</h2>
<p>First published on <a href="https://medium.com/@sue.sk.guo/all-you-should-know-about-translation-equivariance-invariance-in-cnn-cbf2a2ad33cd" target="_blank" rel="noopener noreferrer">Medium</a></p>
<p>Translation invariance and translation equivariance are two important concepts in convolutional neural networks (CNNs) related to the networkâ€™s ability to recognise objects regardless of their position within an image.</p>
<figure><img src="https://cdn-images-1.medium.com/max/2394/1*hF9FN5Ruac76-s5zcfZ0tQ.png" alt="05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube</figcaption></figure>
<h2> Translation invariance</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2092/0*yddB6NGhDosJmw-q" alt="05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube</figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*uZ8kByev1Lc5oiDh2jxr7Q.gif" alt="05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube</figcaption></figure>
<p><strong>Translation invariance</strong> means that a CNN is able to recognise an object in an image regardless of its location or translation within the image. In other words, the networkâ€™s output should remain the same even if the image is shifted or translated in any direction. This property is desirable because it allows the network to generalize well to different images of the same object with different translations.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>Pooling layers help build shift invariance in convolutional networks.</li>
<li>Shift invariance means that the same maximum value will be found under the pooling kernel even if the image is shifted slightly.</li>
<li>However, this shift invariance is only locally true and may not hold if the image is shifted too much.</li>
<li>Pooling is not completely bulletproof with regards to shift invariance, but it can still identify the same features in an image regardless of their position.</li>
</ul>
<h2> Translation equivariance</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2128/0*emgASPn__BRxHofS" alt="05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube</figcaption></figure>
<p>Translation equivariance, on the other hand, means that the networkâ€™s output is related to the location of the object within the image. More specifically, if the input image is shifted or translated, the output of the network will also be shifted or translated accordingly. This property is useful for tasks such as object detection, where the location of the object within the image is important.</p>
<p>In CNNs, translation invariance is achieved through the use of pooling layers, which aggregate <strong>feature maps</strong> into a more compact representation while preserving the most important features. Meanwhile, translation equivariance is achieved through the use of convolutional layers, which apply a filter or kernel to the input image to <strong>extract local features</strong> that are then combined to form a larger, more complex feature map. By using a combination of convolutional and pooling layers, CNNs are able to achieve both translation invariance and equivariance, making them highly effective for image recognition tasks.</p>
<h2> Max Pooling breaks shift equivariance? â€” Try antialiaing</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2000/0*Wyy2QtclkDCkUXbA.gif" alt="Making Convolutional Networks Shift-Invariant Again" tabindex="0" loading="lazy"><figcaption>Making Convolutional Networks Shift-Invariant Again</figcaption></figure>
<p>Convolutional neural networks (CNNs) are approximately shift equivalent through their convolutional layers. However, the use of Max Pooling layers can break shift equivariance (also known as translation equivariance). To address this issue, one solution is to use anti-aliasing techniques in Computer Vision, such as blurring the image and then down-sampling it. This technique can help preserve important features in the image while reducing the effect of minor shifts or translations, which in turn can improve the shift equivariance of the CNN.</p>
<p>You can check more in the <a href="https://richzhang.github.io/antialiased-cnns/" target="_blank" rel="noopener noreferrer">paper</a></p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*unim0oPZtXDmRnWSXz4cyQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>There is another research about <a href="http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html" target="_blank" rel="noopener noreferrer">Harmonic Networks: Deep Translation and Rotation Equivariance</a></p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*wIjQ3UY25Tk6BqHiI_1jZg.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> CNN and Poorly shift invariant</h2>
<p><a href="https://arxiv.org/abs/1805.12177" target="_blank" rel="noopener noreferrer">Why do deep convolutional networks generalize so poorly to small image transformations?</a></p>
<blockquote>
<p>Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the networkâ€™s prediction.
In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that</p>
<ul>
<li>the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem,</li>
<li>and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set.
We discuss two possible solutions to this problem:
(1) antialiasing the intermediate representations and
(2) increasing data augmentation and show that they provide only a partial solution at best.
Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.</li>
</ul>
</blockquote>
<p>There are also some interesting opinions from (Chinese Platform)</p>
<p><a href="https://www.zhihu.com/question/301522740/answer/531606623" target="_blank" rel="noopener noreferrer">Since CNN has translation invariance to images, will it be effective to use image translation (shift) for data augmentation to train CNN?</a></p>
<ul>
<li>It is precisely because pooling itself has weak translation invariance and will lose some information that in tasks that require translation equivariance (such as detection and segmentation), convolutional layers with a stride of 2 are often used instead of pooling layers.</li>
<li>In many classification tasks, global pooling or pyramid pooling is often used at the end of the network to learn global features.</li>
<li>The translation invariance that can be used for classification mainly comes from the parameters. Because of the translation equivalence of convolutional layers, this kind of translation invariance is mainly learned by the final fully connected layer, and it is more difficult for networks without fully connected layers to have this property.</li>
<li>To summarize, the translation invariance of CNN mainly comes from data learning, and the structure can only bring very weak translation invariance, while learning relies on data augmentation.</li>
</ul>
<h3> Reference</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=a4Quhf9NhMY&amp;t=944s" target="_blank" rel="noopener noreferrer">05 Imperialâ€™s Deep learning course: Equivariance and Invariance â€” YouTube</a></li>
<li><a href="https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59" target="_blank" rel="noopener noreferrer">What is translation equivariance, and why do we use convolutions to get it? | by Christian Wolf | Medium</a></li>
<li><a href="https://richzhang.github.io/antialiased-cnns/" target="_blank" rel="noopener noreferrer">Making Convolutional Networks Shift-Invariant Again</a></li>
<li><a href="http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html" target="_blank" rel="noopener noreferrer">Harmonic Networks: Deep Translation and Rotation Equivariance</a></li>
<li><a href="https://www.zhihu.com/question/301522740/answer/531606623" target="_blank" rel="noopener noreferrer">Since CNN has translation invariance to images, will it be effective to use image translation (shift) for data augmentation to train CNN?</a></li>
</ul>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/2394/1*hF9FN5Ruac76-s5zcfZ0tQ.png" type="image/png"/>
    </item>
    <item>
      <title>Tools</title>
      <link>https://suegk.github.io/Algorithm/Conference-Workshop.html</link>
      <guid>https://suegk.github.io/Algorithm/Conference-Workshop.html</guid>
      <source url="https://suegk.github.io/rss.xml">Tools</source>
      <description>Tools AMiner ğŸŒŸ AMiner (aminer.org) aims to provide comprehensive search and mining services for researcher social networks. In this system, we focus on: (1) creating a semantic-based profile for each researcher by extracting information from the distributed Web; (2) integrating academic data (e.g., the bibliographic data and the researcher profiles) from multiple sources; (3) accurately searching the heterogeneous network; (4) analyzing and discovering interesting patterns from the built researcher social network. The main search and analysis functions in AMiner include:</description>
      <pubDate>Mon, 27 Mar 2023 00:43:32 GMT</pubDate>
      <content:encoded><![CDATA[<h1> Tools</h1>
<h2> <a href="https://www.aminer.cn/introduction" target="_blank" rel="noopener noreferrer">AMiner</a> ğŸŒŸ</h2>
<blockquote>
<p><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">AMiner  </a>(<a href="http://aminer.org" target="_blank" rel="noopener noreferrer">aminer.org</a>) aims to provide comprehensive search and mining services for researcher social networks. In this system, we focus on:</p>
<p>(1) creating a semantic-based profile for each researcher by extracting information from the distributed Web;</p>
<p>(2) integrating academic data (e.g., the bibliographic data and the researcher profiles) from multiple sources;</p>
<p>(3) accurately searching the heterogeneous network;</p>
<p>(4) analyzing and discovering interesting patterns from the built researcher social network. The main search and analysis functions in AMiner include:</p>
</blockquote>
<ul>
<li><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">Profile search</a>: input a researcher name (e.g.,<a href="http://aminer.org/profile/jie-tang/53f46a3edabfaee43ed05f08" target="_blank" rel="noopener noreferrer">Jie Tang</a>), the system will return the semantic-based profile created for the researcher using information extraction techniques. In the profile page, the extracted and integrated information include: contact information, photo, citation statistics, academic achievement evaluation, (temporal) research interest, educational history, personal social graph, research funding (currently only US and CN), and publication records (including citation information, and the papers are automatically assigned to several different domains).</li>
<li><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">Expert finding</a>: input a query (e.g., data mining), the system will return experts on this topic. In addition, the system will suggest the top conference and the top ranked papers on this topic. There are two ranking algorithms, VSM and ACT. The former is similar to the conventional language model and the latter is based on our Author-Conference-Topic (ACT) model. Users can also provide feedbacks to the search results.</li>
<li><a href="https://cn.aminer.org/ranks/conf" target="_blank" rel="noopener noreferrer">Conference analysis</a>: input a conference name (e.g., KDD), the system returns who are the most active researchers on this conference, and the top-ranked papers.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Course search</a>: input a query (e.g., data mining), the system will tell you who are teaching courses relevant to the query.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Sub-graph search</a>: input a query (e.g., data mining), the system first tells you what topics are relevant to the query (e.g., five topics "Data mining", "XML Data", "Data Mining / Query Processing", "Web Data / Database design", "Web Mining" are relevant), and then display the most important sub-graph discovered on each relevant topic, augmented with a summary for the sub-graph.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Topic browser</a>: based on our Author-Conference-Topic (ACT) model, we automatically discover 200 hot topics from the publications. For each topic, we automatically assign a label to represent its meanings. Furthermore, the browser presents the most active researchers, the most relevant conferences/papers, and the evolution trend of the topic is discovered.</li>
<li><a href="https://cn.aminer.org/academicstatistics" target="_blank" rel="noopener noreferrer">Academic ranks</a>: we define 8  <a href="http://aminer.org/AcademicStatistics" target="_blank" rel="noopener noreferrer">measures  </a>to evaluate the researcher's achievement. The  <a href="http://aminer.org/AcademicStatistics" target="_blank" rel="noopener noreferrer">measures  </a>include "h -index", "Citation", "Uptrend, "Activity", "Longevity", "Diversity, "Sociability", "New Star". For each measure, we output a ranking list in different domains. For example, one can search who have the highest citation number in the "data mining" domain.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">User management</a>: one can register as a user to: (1) modify the extracted profile information; (2) provide feedback on the search results; (3) follow researchers in AMiner; (4) create an AMiner page (which can be used to advertise confs/workshops, or recruit students).</li>
</ul>
<figure><img src="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/X64Hh3.png" alt="X64Hh3" tabindex="0" loading="lazy"><figcaption>X64Hh3</figcaption></figure>
<figure><img src="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/HXLtQX.png" alt="HXLtQX" tabindex="0" loading="lazy"><figcaption>HXLtQX</figcaption></figure>
<p>Keep you updated with state-of-the-art technology is really important in the Machine learning and deep learning field.</p>
<p>There are some tips from Andre NG on how to read research papers. <a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>So, where can we find the popular papers?</p>
<h2> Paperswithcode â€” <a href="https://paperswithcode.com/sota" target="_blank" rel="noopener noreferrer">Browse State-of-the-Art</a></h2>
<p>The papers are well categorized so you can follow what Andre said, choose an area of interest like semantic segmentation, and read 15â€“20 papers to get a good understanding of this field. More importantly, you can find the paperâ€™s code.</p>
<p><img src="https://miro.medium.com/max/1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1048/1*GFD5JPhEEXzp7vwMmKfpjw.png" alt="" loading="lazy"></p>
<h2> <a href="https://nn.labml.ai/index.html" target="_blank" rel="noopener noreferrer">labml.ai Deep Learning Paper Implementations</a></h2>
<p>59 Implementations/tutorials of deep learning papers with side-by-side notes ğŸ“; including transformers (original, xl, switch, feedback, vit, â€¦), optimizers (adam, adabelief, â€¦), gans(cyclegan, stylegan2, â€¦), ğŸ® reinforcement learning (ppo, dqn), capsnet, distillation, â€¦ ğŸ§ </p>
<p>This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations. So you are able to read the paper while understanding how to implement it by Pytorch.</p>
<figure><img src="https://miro.medium.com/max/1400/1*QI_S19lNihupZgpJj7eMfg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="http://labml.ai" target="_blank" rel="noopener noreferrer">labml.ai</a> â€” <a href="https://papers.labml.ai/" target="_blank" rel="noopener noreferrer">Trending Research Papers</a></h2>
<p>The most popular research papers on social media like Twitter. You can easily find links to download papers, paper summaries, explanation videos, and discussions.</p>
<p><img src="https://miro.medium.com/max/1400/1*7pr_Bk_Sh38aNgmzOPgb1A.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*XJkaJCU4qvndNY1pUV5VZw.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*MxxeMhbirfw3CcN9Vs4eRg.png" alt="" loading="lazy"></p>
<p>The chrome extension is really helpful as well.</p>
<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>This extension shows you the following details about research papers:
âœ¨ 2-line summary
âœ¨ Availability source code, videos, and discussions
âœ¨ Popularity on Twitter
âœ¨ Conferences

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> <a href="https://aman.ai/papers/#noise-contrastive-estimation-a-new-estimation-principle-for-unnormalized-statistical-models" target="_blank" rel="noopener noreferrer">Paper lists made by Aman</a></h2>
<p>A summary of key papers in Computer Vision, NLP, and Speech recognition.</p>
<figure><img src="https://miro.medium.com/max/1400/1*BdkxFnegXS0g0sHUNW-Akw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="https://deeplearn.org/" target="_blank" rel="noopener noreferrer">Deep Learning Monitor</a></h2>
<p>Another website where you can find the hot papers on social media.</p>
<p>The great feature is that you can create some monitors with the keywords related to the topic of interest and check new updates every one or two weeks. Once you find a good paper and you log in Mendeley on this website, you can directly send it to your Mendeley account.</p>
<p><img src="https://miro.medium.com/max/1400/1*3HtQpm1hSI6ApIAIOl3YvQ.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*cz1BAvWn42uztuayw0dm-w.png" alt="" loading="lazy"></p>
]]></content:encoded>
      <enclosure url="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/X64Hh3.png" type="image/png"/>
    </item>
  </channel>
</rss>