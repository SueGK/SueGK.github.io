<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <atom:link href="https://suegk.github.io/rss.xml" rel="self" type="application/rss+xml"/>
    <title>Digest Today</title>
    <link>https://suegk.github.io/</link>
    <description>Artificial Intelligence | Deep Learning | Productivity Software | Note-Taking</description>
    <language>en-US</language>
    <pubDate>Wed, 28 Aug 2024 04:57:20 GMT</pubDate>
    <lastBuildDate>Wed, 28 Aug 2024 04:57:20 GMT</lastBuildDate>
    <generator>vuepress-plugin-feed2</generator>
    <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
    <category>效率工具</category>
    <category>ML</category>
    <category>学习资源</category>
    <category>Pytorch</category>
    <category>CV</category>
    <item>
      <title>效率工具：Roam Research中Discourse Graph插件使用流程</title>
      <link>https://suegk.github.io/posts/2022-06-30.html</link>
      <guid>https://suegk.github.io/posts/2022-06-30.html</guid>
      <source url="https://suegk.github.io/rss.xml">效率工具：Roam Research中Discourse Graph插件使用流程</source>
      <description>I’m still surprised how much Roam Research potential would you find! Have been trying to use roam for my machine learning study for 2 weeks. The combination of discourse graph and query builder in @roam_js is really powerful! Here are what I found useful:我仍然很惊讶你会发现 Roam Research 的潜力有多大！2 周以来，我一直在尝试使用 roam 进行我的机器学习学习。@roam_js&amp;nbsp;中 discourse graph 和 query builder 的组合真的很强大！以下是我发现有用的内容：</description>
      <category>效率工具</category>
      <pubDate>Thu, 30 Jun 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>I’m still surprised how much Roam Research potential would you find! Have been trying to use roam for my machine learning study for 2 weeks. The combination of discourse graph and query builder in <a href="https://twitter.com/roam_js" target="_blank" rel="noopener noreferrer">@roam_js</a> is really powerful! Here are what I found useful:我仍然很惊讶你会发现 Roam Research 的潜力有多大！2 周以来，我一直在尝试使用 roam 进行我的机器学习学习。@roam_js&nbsp;中 discourse graph 和 query builder 的组合真的很强大！以下是我发现有用的内容：</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*GHCaGMokzGHolVp-.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*KwhYhr0UaYP9XO16.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><a href="https://medium.com/@sue.sk.guo/((WjDE4sICN))" target="_blank" rel="noopener noreferrer">*</a> is the alias for the code node.*）是 Code 节点的别名。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:318/0*32JqBwp8RBRywxHu.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The reason why I add both code node and alias for that code node is that Discourse can’t recognize a block reference of a code node as code node to establish the relationship. So like the gif below, no discourse relation found.我之所以同时为该代码节点添加 code node 和 alias，是因为 Discourse 无法将代码节点的块引用识别为代码节点来建立关系。所以就像下面的 gif 一样，没有找到 discourse 关系。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*uJWjYEvnPgi5fohx.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*KpaOvhdCiabj25YY.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Now, I can easily go to the code node original block by clicking the * and then go back by checking the reference number.现在，我可以通过单击 * 轻松转到代码节点原始块，然后通过检查参考编号返回。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*km3RxfLN8xIbtsh8.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/0*GHCaGMokzGHolVp-.png" type="image/png"/>
    </item>
    <item>
      <title>效率工具：Roam Research代办任务管理流程</title>
      <link>https://suegk.github.io/posts/2022-07-08.html</link>
      <guid>https://suegk.github.io/posts/2022-07-08.html</guid>
      <source url="https://suegk.github.io/rss.xml">效率工具：Roam Research代办任务管理流程</source>
      <description>For Blogs that need a lot of time to read and learn对于需要大量时间阅读和学习的博客 (such as code projects etc)（如代码项目等） ▶ Using Blog Reference like ((Blog)) to gather all blog/articles so I could easily check in the “Reading&amp;Watching List” Page▶ 使用像&amp;nbsp;（（Blog））&amp;nbsp;这样的博客参考来收集所有博客/文章，这样我就可以轻松查看“阅读&amp;观看列表”页面</description>
      <category>效率工具</category>
      <pubDate>Fri, 08 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p><strong>For Blogs that need a lot of time to read and learn对于需要大量时间阅读和学习的博客
(such as code projects etc)（如代码项目等）</strong></p>
<p>▶ Using Blog Reference like <strong>((Blog))</strong> to gather all blog/articles so I could easily check in the “Reading&amp;Watching List” Page▶ 使用像&nbsp;（（Blog））&nbsp;这样的博客参考来收集所有博客/文章，这样我就可以轻松查看“阅读&amp;观看列表”页面</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*7NRdxjiFjzSzeAom" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/0*H1HNTw7kJAXXSNke" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><strong>For smaller todo list&nbsp;对于较小的待办事项列表</strong></p>
<p>I just use roam default /todo(in any pages) and use roam/js smartblocks TODO commands to fetch a list of block references of TODOs for Today/Overdue/Future TODOS我只使用 roam default /todo（在任何页面中）并使用 roam/js smartblocks TODO 命令来获取今天/逾期/未来 TODOS 的 TODO 的块引用列表</p>
<p>My Usage Scenarios: <strong>Morning Journal</strong>我的使用场景：晨报</p>
<p>When I start to write Morning Journal, I’ll use #42SmartBlock to automatically create the journal with <strong>Get Thing Done</strong> section where it shows all the TODOs for Today/Overdue/Future TODOS. Therefore I could easily check up the random TODOs to find somethings you’d like to do today and add them to <strong>Make time for today’s Highlight</strong> section. I also have projects pages where contains all todo for the specific project but sometimes it’s convenient and mindless to just fetch some random todo blocks.当我开始编写晨间日记时，我将使用 #42SmartBlock 自动创建带有&nbsp;Get Thing Done&nbsp;部分的日记，其中显示今天/逾期/未来 TODO 的所有 TODO。因此，我可以轻松地检查随机的 TODO，找到您今天想做的事情，并将它们添加到&nbsp;Make time for today's Highlight&nbsp;部分。我也有项目页面，其中包含特定项目的所有待办事项，但有时只是获取一些随机的待办事项块既方便又无意识。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1064/1*xkkFs4Y29utCd7jBnYS0Cg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1136/1*9NOdgK41HM8KK2hbvZtECg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/0*7NRdxjiFjzSzeAom" type="image/"/>
    </item>
    <item>
      <title>ML公式推导 — Logistic Regression Gradient Descent Derivatives</title>
      <link>https://suegk.github.io/posts/2022-07-11.html</link>
      <guid>https://suegk.github.io/posts/2022-07-11.html</guid>
      <source url="https://suegk.github.io/rss.xml">ML公式推导 — Logistic Regression Gradient Descent Derivatives</source>
      <description>From Andrew Ng</description>
      <category>ML</category>
      <pubDate>Mon, 11 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<figure><img src="https://cdn-images-1.medium.com/max/3792/1*psolPGqHQtCuVdAsW0TE7A.png" alt="From Andrew Ng" tabindex="0" loading="lazy"><figcaption>From Andrew Ng</figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/3822/1*I-xYOKDargu0U75QkvoS_A.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/3792/1*psolPGqHQtCuVdAsW0TE7A.png" type="image/png"/>
    </item>
    <item>
      <title>效率工具：书简管理器Raindrop</title>
      <link>https://suegk.github.io/posts/2022-07-25.html</link>
      <guid>https://suegk.github.io/posts/2022-07-25.html</guid>
      <source url="https://suegk.github.io/rss.xml">效率工具：书简管理器Raindrop</source>
      <description>Tools:&amp;nbsp;工具： @raindrop_io : bookmark manager/read later/highlighting@raindrop_io&amp;nbsp;： 书签管理器/稍后阅读/高亮显示 @IFTTT : automation&amp;workflow@IFTTT&amp;nbsp;： 自动化&amp;工作流 You can customize the automation on ifttt. I normally choose the collection folder and tags.您可以在 ifttt 上自定义自动化。我通常会选择收藏文件夹和标签。</description>
      <category>效率工具</category>
      <pubDate>Mon, 25 Jul 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<h1> Tools:&nbsp;工具：</h1>
<p><a href="http://twitter.com/raindrop_io" target="_blank" rel="noopener noreferrer">@raindrop_io</a> : bookmark manager/read later/highlighting@raindrop_io&nbsp;： 书签管理器/稍后阅读/高亮显示
<a href="http://twitter.com/IFTTT" target="_blank" rel="noopener noreferrer">@IFTTT</a> : automation&amp;workflow@IFTTT&nbsp;： 自动化&amp;工作流
You can customize the automation on ifttt. I normally choose the collection folder and tags.您可以在 ifttt 上自定义自动化。我通常会选择收藏文件夹和标签。</p>
<p>Raindrop is a bookmark manager which is able to collect everything you come across while browsing such as articles, tweets, YouTube videos and so on.Raindrop 是一个书签管理器，它能够收集您在浏览时遇到的所有内容，例如文章、推文、YouTube 视频等。</p>
<h1> Supported File Types支持的文件类型</h1>
<ul>
<li>Images (<code>jpeg</code>, <code>gif</code>, <code>png</code>)图片（jpeg、gif、png）</li>
<li>Videos (<code>mp4</code>, <code>mov</code>, <code>wmv</code>, <code>webm</code>)视频（mp4、mov、wmv、webm）</li>
<li>Documents (<code>pdf</code>, <code>doc</code>, <code>docx</code>, <code>xls</code>, <code>xlsx</code>, <code>txt</code>)文档 （pdf，&nbsp;doc， docx，&nbsp;xls， xlsx，&nbsp;txt）</li>
</ul>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*TMjVWkF1M66mQot_o9SNFQ.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>from <a href="http://Raindrop.io" target="_blank" rel="noopener noreferrer">Raindrop.io</a> website&nbsp;从 <a href="http://Raindrop.io" target="_blank" rel="noopener noreferrer">Raindrop.io</a> 网站</p>
<p>It has many powerful features.它有许多强大的功能。</p>
<h1> 1. tags&amp;filters&nbsp;1. 标签和过滤器</h1>
<p>You can search your collected website by tags or filters.您可以按标签或过滤器搜索您收集的网站。
You can tag items and filter items with specific tags in specific collection folders.您可以在特定集合文件夹中使用特定标签标记项目和筛选项目。
Filters automatically gather different types of bookmarks including Highights/Links/Articles/Video/Documents/Without lags.过滤器会自动收集不同类型的书签，包括 Highights/Links/Articles/Video/Documents/Without lags。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1398/1*iD5GOFP39K8WrIkCxr7XEw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1656/1*zm0vC8DtiyG-TGdI-VgFBw.jpeg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h1> 2. Integrations — Automation2. 集成 — 自动化</h1>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*oQqf9OV6NQdBhAbvCn3UrA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1276/1*30a9Fi13TmR0-zQD3QeTlw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>from <a href="http://raindrop.io" target="_blank" rel="noopener noreferrer">raindrop.io</a> website&nbsp;从 <a href="http://raindrop.io" target="_blank" rel="noopener noreferrer">raindrop.io</a> 网站</p>
<p>You can customize the automation on ifttt. I normally choose the collection folder and tags. You could automately collect liked tweets, liked YouTube videos and Medium Bookmarks into Raindrop.您可以在 ifttt 上自定义自动化。我通常会选择收藏文件夹和标签。您可以自动将喜欢的推文、喜欢的 YouTube 视频和 Medium 书签收集到 Raindrop 中。</p>
<p>Here are my IFTTT settings:以下是我的 IFTTT 设置：</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:514/0*yzcDMFEpKdMIIeYK.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Create Twitter/Medium folders
创建 Twitter/Medium 文件夹</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:332/0*recwsFiMLJUOjKCC.jpg" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>customized in IFTTT&nbsp;在 IFTTT 中定制</p>
<ol start="3">
<li>Highlighting&nbsp;3. 突出显示</li>
</ol>
<p>You can highlight the articles you saved in raindrop and export the highlights.您可以突出显示保存在 raindrop 中的文章并导出突出显示。</p>
<p>The IMPORTANT THING IS!!!重要的是!!</p>
<blockquote>
<p>There’s no limit on total number of pages and highlights.页面总数和 highlights 总数没有限制。</p>
</blockquote>
<p>There are still more powerful features you could explore in their website!您还可以在他们的网站中探索更强大的功能！</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*Otyg6Anyd4qtgV3f-BwTWA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1400/1*TMjVWkF1M66mQot_o9SNFQ.gif" type="image/gif"/>
    </item>
    <item>
      <title>如何改变GitHub中照片的位置</title>
      <link>https://suegk.github.io/posts/2022-08-16.html</link>
      <guid>https://suegk.github.io/posts/2022-08-16.html</guid>
      <source url="https://suegk.github.io/rss.xml">如何改变GitHub中照片的位置</source>
      <description>You need to use some HTML in your Markdown. You could also use some text expander app to type it more quickly. If you prefer video, here is my Youtube video! Video Link Here is my Macro for Keyboard Maestro which automate applications or web sites, text or images, simple or complex, on command or scheduled. You can also use other software such as TextExpander, ProKeys chrome extension, Alfred Snippets etc.</description>
      <category>学习资源</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>You need to use some HTML in your Markdown. You could also use some text expander app to type it more quickly.</p>
<p>If you prefer video, here is my Youtube video! <a href="https://www.youtube.com/watch?v=8LKybiFA-is" target="_blank" rel="noopener noreferrer">Video Link</a></p>
<p>Here is my Macro for <a href="https://www.keyboardmaestro.com/main/" target="_blank" rel="noopener noreferrer">Keyboard Maestro</a> which automate applications or web sites, text or images, simple or complex, on command or scheduled. You can also use other software such as TextExpander, ProKeys chrome extension, Alfred Snippets etc.</p>
<p>Please download it here: <a href="https://forum.keyboardmaestro.com/t/resize-image-and-change-location-in-github/28632" target="_blank" rel="noopener noreferrer">link</a></p>
<ul>
<li>Resize pictures</li>
<li>Change location of pictures</li>
<li>Add many pictures in one row</li>
</ul>
<h2> Resize pictures</h2>
<p>You can custimize the width and height.</p>
<pre><code>`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;img src="yourPictureURL.jpg" width="400" height="200"/&gt;`
</code></pre>
<p>Before: Markdown</p>
<pre><code>![]([https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png](https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png))
</code></pre>
<p>After: HTML</p>
<pre><code>&lt;img src=”[https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png](https://user-images.githubusercontent.com/71711489/183897714-9e8b8508-2bc3-4bfd-b61e-600c4bd47711.png)" width=”400"&gt;
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*4G0C2BUOOMRO_CayzpqjvA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Change location of resized pictures</h2>
<p>Put the picture on the right:</p>
<pre><code>`&lt;div align="right"&gt;`
`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;/div&gt;`
</code></pre>
<p>Put the picture on the center:</p>
<pre><code>`&lt;div align="center"&gt;`
`&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
`&lt;/div&gt;`
</code></pre>
<p>If you don’t want to resize pictures then you can remove width="400" .</p>
<figure><img src="https://cdn-images-1.medium.com/max/2076/1*4Fo9_Js2ZV91cWDCqrOsaw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Add many pictures in one row</h2>
<pre><code>`&lt;p float="left"&gt;`
 `&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
 `&lt;img src="yourPictureURL.jpg" width="400"/&gt;`
 .....
`&lt;/p&gt;`
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*_DI25TlR2A3aLOb4O5UpEw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> Macro Download</h2>
<p>I made some Macro to easily type the HTML code.</p>
<p><strong>First, get the image url.</strong>
I’m using an file hosting tool called <a href="https://apps.apple.com/us/app/upic-hosting-tool/id1510718678" target="_blank" rel="noopener noreferrer">uPic</a> to get image url. You can also upload images in your GitHub to get image url.</p>
<p><strong>Second, Typed shortcuts are:</strong></p>
<pre><code>?zz  # image resize
?zc  # image resize and center location
?zr  # image resize and right location
?zm  # multiple image in one row
</code></pre>
<p>You can also change to hot key shortcuts or just call macro directly.</p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*V4MmQDaXqW6wWI3mcRvx8w.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/3160/1*WuFAjNBSaPeSNScNInF-hg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*fvGFEbZHbIZpi5qoDsrDAA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
    </item>
    <item>
      <title>发现热门 ML/DL 研究论文的网站</title>
      <link>https://suegk.github.io/posts/2022-08-18-1.html</link>
      <guid>https://suegk.github.io/posts/2022-08-18-1.html</guid>
      <source url="https://suegk.github.io/rss.xml">发现热门 ML/DL 研究论文的网站</source>
      <description>Keep you updated with state of the art technology is really important in the Machine learning and deep learning field. 让您了解最先进的技术在机器学习和深度学习领域非常重要。 There are some tips from Andre NG on how to read research papers. https://www.youtube.com/watch?v=733m6qBH-jI</description>
      <category>学习资源</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>Keep you updated with state of the art technology is really important in the Machine learning and deep learning field.</p>
<p>让您了解最先进的技术在机器学习和深度学习领域非常重要。</p>
<p>There are some tips from Andre NG on how to read research papers. <a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>Andre NG 提供了一些关于如何阅读研究论文的建议。</p>
<p><a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>So, where can we find the popular papers?那么，我们在哪里可以找到热门论文呢？=</p>
<h2> * paperswithcode — <a href="https://paperswithcode.com/sota" target="_blank" rel="noopener noreferrer">Browse State-of-the-Art</a></h2>
<p>The papers are well categorized so you can follow what =Andre =said, choose an area of interest like semantic segmentation, and read 15–20 papers to get a good understanding of this field. More importantly, you can find the paper’s code.这些论文分类很好，因此您可以按照&nbsp;Andre&nbsp;所说的内容，选择一个感兴趣的领域，例如语义分割，并阅读 15-20 篇论文以很好地理解该领域。更重要的是，您可以找到论文的代码。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1048/1*GFD5JPhEEXzp7vwMmKfpjw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> * <a href="https://nn.labml.ai/index.html" target="_blank" rel="noopener noreferrer">labml.ai Deep Learning Paper Implementations</a></h2>
<p>59 Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, …), optimizers (adam, adabelief, …), gans(cyclegan, stylegan2, …), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, … 🧠59 深度学习论文的实现/教程，并排注释📝;包括 Transformer （Original， XL， Switch， Feedback， VIT等）， 优化器 （Adam， AdaBelief， ...）， GANS（Cyclegan， StyleGan2， ...）， 🎮 强化学习 （PPO， DQN）， CapsNet， Distillation ...🧠</p>
<p>This is a collection of simple PyTorch implementations of neural networks and related algorithms.  These implementations are documented with explanations. So you are able to read the paper while understanding how to implement it by Pytorch.这是神经网络和相关算法的简单 PyTorch 实现的集合。<mark>这些实现记录了说明。因此，您可以在阅读本文的同时了解如何通过 Pytorch 实现它。</mark></p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*QI_S19lNihupZgpJj7eMfg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> * <a href="http://labml.ai" target="_blank" rel="noopener noreferrer">labml.ai</a> — <a href="https://papers.labml.ai/" target="_blank" rel="noopener noreferrer">Trending Research Papers</a></h2>
<p>The most popular research papers on social media like Twitter. You can easily find links to download papers, paper summaries, explanation videos, and discussions.Twitter 等社交媒体上最受欢迎的研究论文。您可以轻松找到下载论文、论文摘要、解释视频和讨论的链接。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*7pr_Bk_Sh38aNgmzOPgb1A.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*XJkaJCU4qvndNY1pUV5VZw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*MxxeMhbirfw3CcN9Vs4eRg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The chrome extension is really helpful as well.chrome 扩展程序也非常有用。</p>
<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>This extension shows you the following details about research papers:
✨ 2-line summary
✨ Availability source code, videos, and discussions
✨ Popularity on Twitter
✨ Conferences
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> * <a href="https://aman.ai/papers/#noise-contrastive-estimation-a-new-estimation-principle-for-unnormalized-statistical-models" target="_blank" rel="noopener noreferrer">Paper lists made by Aman</a></h2>
<p>A summary of key papers in Computer Vision, NLP, and Speech recognition.</p>
<p>计算机视觉、NLP 和语音识别方面的关键论文摘要。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*BdkxFnegXS0g0sHUNW-Akw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="https://deeplearn.org/" target="_blank" rel="noopener noreferrer">Deep Learning Monitor</a></h2>
<p>=Another website where you can find the hot papers on social media.另一个您可以在社交媒体上找到热门论文的网站。=</p>
<p>The great feature is that you can create some monitors with the keywords related to the topic of interest and check new updates every one or two weeks. Once you find a good paper and you log in Mendeley on this website, you can directly send it to your Mendeley account.很棒的功能是您可以使用与感兴趣主题相关的关键字创建一些监视器，并每隔一两周检查一次新的更新。一旦你找到一篇好的论文并在本网站上登录 Mendeley，你就可以直接将其发送到你的 Mendeley 帐户。</p>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*3HtQpm1hSI6ApIAIOl3YvQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<figure><img src="https://miro.medium.com/v2/resize:fit:1400/1*cz1BAvWn42uztuayw0dm-w.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
]]></content:encoded>
      <enclosure url="https://miro.medium.com/v2/resize:fit:1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" type="image/png"/>
    </item>
    <item>
      <title>OpenCV-color in BGR order you must know</title>
      <link>https://suegk.github.io/posts/2022-08-18.html</link>
      <guid>https://suegk.github.io/posts/2022-08-18.html</guid>
      <source url="https://suegk.github.io/rss.xml">OpenCV-color in BGR order you must know</source>
      <description>Wanna play around with the code? Link import os import numpy as np import argparse import cv2 import matplotlib.pyplot as plt</description>
      <category>学习资源</category>
      <pubDate>Thu, 18 Aug 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<p>Wanna play around with the code? <a href="https://github.com/SueGK/Courses/blob/main/pyimagesearch-opencv-17-day-course/OpenCV-Mynotes/opencv_BGR_color.ipynb" target="_blank" rel="noopener noreferrer">Link</a></p>
<pre><code>import os
import numpy as np
import argparse
import cv2
import matplotlib.pyplot as plt
</code></pre>
<p>The original picture looks like:</p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/0*gMD-zYzvJWZegLIH.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>The OpenCV assumes images are in BGR channel order. OpenCV imread, imwrite and imshow all work with the BGR order, so the image won't change if we use cv2.imshow to show the image. But it doesn't work with matplotlib.</p>
<p>Most image processing library use the RGB ordering such as matplotlib so if use plt.imshow, the color of the logo changed.</p>
<pre><code>img = cv2.imread("logo.png")

# show the image by cv2
# The cv2.imshow() and cv.imshow() functions from the opencv-python package are incompatible with Jupyter notebook;
# see https://github.com/jupyter/notebook/issues/3935.
# As a replacement, you can use the following function:
from google.colab.patches import cv2_imshow
cv2_imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*Ixo3Bu6gZwaoAGhT64Rlyg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code># show the image by matplotlib
plt.subplot(111)
plt.imshow(img)
plt.title("Original")
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*sqd7CCalX7L04M21PYdCvA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>If we want to read image in RGB order in OpenCV, we can use:</p>
<p>img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</p>
<pre><code># read images in RGB order in OpenCV
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
from google.colab.patches import cv2_imshow
cv2_imshow(img_rgb)
plt.imshow(img_rgb)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*b6K55FOZwp3tePqYe-DERg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p><strong>cv2.imread(FILENAME, FLAG)</strong></p>
<p>There are some flags:</p>
<p><strong>cv2.IMREAD_UNCHANGED:</strong> read the image as is from the source(with alpha channel). If the source image is an RGB, it loads the image into array with Red, Green and Blue channels.</p>
<p><strong>cv2.IMREAD_COLOR:</strong> convert image to the 3 channel BGR color image but no transparency channel</p>
<p><strong>cv2.IMREAD_GRAYSCALE:</strong> convert image to the single channel grayscale image</p>
<p>Check more flags here: <a href="https://docs.opencv.org/3.4/d8/d6a/group__imgcodecs__flags.html#ga61d9b0126a3e57d9277ac48327799c80" target="_blank" rel="noopener noreferrer">OpenCV: Flags used for image file reading and writing</a></p>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_UNCHANGED)
cv2_imshow(img)
plt.imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*d_zvffWdR2cnGDKDOuG0DA.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_COLOR)
cv2_imshow(img)
plt.imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*bz2dYJPqveXPof4lw4f24A.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_GRAYSCALE)
cv2_imshow(img)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*yYNiNY657JdYVMZMLnqlVg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>Let's have a look at the BGR channel. If we want to print the whole array then set np.set_printoptions(threshold=np.inf) and set it back to default after print the array. np.set_printoptions(threshold=1000</p>
<pre><code>img = cv2.imread('logo.png', cv2.IMREAD_COLOR)
cv2_imshow(img)
b = img[:,:,0] # get blue channel
g = img[:,:,1] # get green channel
r = img[:,:,2] # get red channel

print(b)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*aLUmFMIu3E8WJ6zMM0_t8g.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>np.set_printoptions(threshold=np.inf)
print(b)
np.set_printoptions(threshold=1000)

# the output is really long so I won't print it here.
</code></pre>
<p>Now let's convert it to <strong>RGB</strong> channel. Even though we read image in RGB order but the logo color will change if we use cv2.imshow to show the image because it works with <strong>BGR</strong> order. plt.imshow show the original color of the image because it works with <strong>RGB</strong> order as well.</p>
<pre><code># read images in RGB order in OpenCV
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
from google.colab.patches import cv2_imshow
cv2_imshow(img_rgb)
plt.imshow(img_rgb)
</code></pre>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*abBgvTaTAg_wKCYiWBMuPQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<pre><code>r1 = img_rgb[:,:,0] # get blue channel
g1 = img_rgb[:,:,1] # get green channel
b1 = img_rgb[:,:,2] # get red channel

(img[:,:,0] == img_rgb[:,:,2]).all()

Output: True
</code></pre>
<p>When you try to draw a rectangle box in OpenCV, be careful that the color of rectangle is also in <strong>BGR</strong> order.</p>
<pre><code># read the image
image = cv2.imread('logo.png')

# represents the top left corner of rectangle
start_point = (5, 5)

# represents the bottom right corner of rectangle
end_point = (20, 20)

# choose the rectangle color in BGR
color = (0, 0, 255) # red

# thickness of lines that make up the rectangle is 2 px
thickness = 2

# draw a rectangle with red line borders of thickness of 2 px
image = cv2.rectangle(image, start_point, end_point, color, thickness)

# Displaying the image
cv2_imshow(image)
</code></pre>
<p><img src="https://cdn-images-1.medium.com/max/2000/1*J3rSiqjnPRM1D04oPitI1Q.png" alt="" loading="lazy">
<a href="https://medium.com/mlearning-ai/mlearning-ai-submission-suggestions-b51e2b130bfb" target="_blank" rel="noopener noreferrer"><strong>Mlearning.ai Submission Suggestions</strong>
<em>How to become a writer on Mlearning.ai</em>medium.com</a></p>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/2000/0*gMD-zYzvJWZegLIH.png" type="image/png"/>
    </item>
    <item>
      <title>Pytorch DataLoader使用指南</title>
      <link>https://suegk.github.io/posts/2022-12-06.html</link>
      <guid>https://suegk.github.io/posts/2022-12-06.html</guid>
      <source url="https://suegk.github.io/rss.xml">Pytorch DataLoader使用指南</source>
      <description>by Sue Create a custom dataset To create a customized dataset in PyTorch, you should create a subclass of the torch.utils.data.Dataset class and implement the following methods:</description>
      <category>Pytorch</category>
      <pubDate>Tue, 06 Dec 2022 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<figure><img src="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" alt="by Sue" tabindex="0" loading="lazy"><figcaption>by Sue</figcaption></figure>
<h2> <strong>Create a custom dataset</strong></h2>
<p>To create a customized dataset in PyTorch, you should create a subclass of the torch.utils.data.Dataset class and implement the following methods:</p>
<ul>
<li><strong>init</strong>(): This method should initialize the dataset and any instance variables that you need. It should accept any arguments necessary to create the dataset.</li>
<li><strong>len</strong>(): This method should return the length of the dataset (i.e. the number of samples in the dataset).</li>
<li><strong>getitem</strong>(idx): This method should return the data for the sample with the specified index. The returned data should be a tuple containing the data and the corresponding label for the sample.</li>
</ul>
<p>Here is an example of how your custom dataset class might look:</p>
<pre><code>import torch
from torch.utils.data import Dataset

class CustomDataset(Dataset):
    def__init__(self, data, labels):
        self.data = data
        self.labels = labels

def__len__(self):
        return len(self.labels)

def__getitem__(self, idx):
        return self.data[idx], self.labels[idx]
</code></pre>
<p>To use your custom dataset, you can create an instance of the class and then pass it to a PyTorch DataLoader object. The DataLoader can then be used to iterate over the dataset and retrieve the samples.</p>
<pre><code># Create an instance of the CustomDataset class
dataset = CustomDataset(data, labels)

# Create a DataLoader for the dataset
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

# Loop over the dataloader to retrieve the samples
for data, labels in dataloader:
    # Do something with the data and labels
</code></pre>
<p>You can learn more about creating custom datasets in PyTorch in the official PyTorch documentation: <a href="https://pytorch.org/docs/stable/data.html#creating-custom-datasets" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/data.html#creating-custom-datasets</a></p>
<h2> <strong>what does dataloader do?</strong></h2>
<p>A PyTorch DataLoader is an object that provides a number of benefits when working with large datasets. It is typically used in conjunction with a Dataset object that provides the data that the DataLoader will iterate over.</p>
<p>The main purpose of the DataLoader is to batch the data from the Dataset and provide it to the model during training. It allows you to specify the batch size and whether or not the data should be shuffled each epoch. This can be useful for training models on large datasets that don't fit in memory, as it allows the data to be processed in smaller batches.</p>
<p>In addition to batching the data, the DataLoader can also be used to perform data augmentation and preprocessing on the fly. This can be useful for tasks like image classification, where you may want to apply random transformations to the images in each batch to improve the model's generalization ability.</p>
<p>Overall, the DataLoader is an important part of the PyTorch data processing pipeline, and is typically used when training deep learning models on large datasets. It provides an iterator over a dataset, allowing you to train your model on a large dataset by loading only a small portion of the data into memory at a time.</p>
<p>The DataLoader class takes a dataset and a batch size as input and returns an iterator over the dataset that yields mini-batches of data. You can use the DataLoader class to shuffle the data and define the number of workers that will be used to load the data in parallel.</p>
<p>One way to use the DataLoader class is with the for loop. Here is an example:</p>
<pre><code># Import the DataLoader class
from torch.utils.data import DataLoader

# Create a dataset
dataset = SomeDataset()

# Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Use the DataLoader object like an iterator
for data in data_loader:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<p>The DataLoader class also has a <strong>iter</strong> method that returns an iterator over the dataset, allowing you to use the DataLoader instance in a for loop.</p>
<pre><code># Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, shuffle=True)

# Get an iterator over the dataset
data_iterator = iter(data_loader)

# Use the iterator in a for loop
for data in data_iterator:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<h2> <strong>collate_fn in DataLoader</strong></h2>
<p>The DataLoader class also provides a way to customize the way data is loaded by defining a collate_fn function. The collate_fn function defines how the data will be combined into a mini-batch. This is useful when your dataset contains data of different sizes, such as images of different sizes.</p>
<p>Here is an example of using a collate_fn function:</p>
<pre><code># Import the DataLoader class
from torch.utils.data import DataLoader

# Create a dataset
dataset = SomeDataset()

# Define a collate function
def collate_fn(data):
    # Sort the data in descending order of length
    data.sort(key=lambda x: len(x[0]), reverse=True)

# Unpack the data
    inputs, labels = zip(*data)

# Pad the inputs
    inputs = pad_sequence(inputs, batch_first=True)

return inputs, labels

# Create a DataLoader instance
data_loader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)

# Use the DataLoader object like an iterator
for data in data_loader:
    # Get the data
    inputs, labels = data

# Use the data to train your model
    train(model, inputs, labels)
</code></pre>
<p><strong>Flowchat: How does DataLoader process data?</strong></p>
<figure><img src="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" alt="by Sue" tabindex="0" loading="lazy"><figcaption>by Sue</figcaption></figure>
<pre><code>num_dataset = 160
batch_size = 16
iteration = num_dataset / batch_size = 10 for i, data in enumerate(train_loader):
inputs, labels = data
</code></pre>
<p>When using a DataLoader instance in PyTorch, you can iterate over it in a for loop to retrieve the data in mini-batches.</p>
<p>In this example, the DataLoader instance will use a DataLoaderIter instance to load the data. The DataLoaderIter instance will call the Sampler to generate a list of indices that specify which elements from the dataset should be included in the mini-batch. The DataLoaderIter will then use a DatasetFetcher instance to retrieve the data from the dataset using the generated indices.</p>
<p>The DatasetFetcher instance will call the <strong>getitem</strong> method of the Dataset to retrieve the data for each index in the list of indices. This will return a list of data samples, where each sample is a tuple containing the input data and the corresponding label.</p>
<p>Finally, the DataLoaderIter will use the collate_fn function to combine the data samples into a mini-batch. The collate_fn function can be customized to define how the data will be combined into a mini-batch. The output of the collate_fn function is a mini-batch of data that is ready to be used by your model for training or evaluation.</p>
<p>In summary, when using a DataLoader instance in PyTorch,</p>
<ul>
<li>The resulting mini-batch of data is then yielded by the DataLoader instance in a for loop, and then decide whether to use a single or multi-process DataLoaderIter depending on whether multi-processing is used.</li>
<li>the data is loaded in mini-batches using a Sampler to generate indices</li>
<li>a DatasetFetcher to retrieve the data from the dataset based on the indices. In the DatasetFetcher, the <strong>getitem</strong>() method of the Dataset is called to get the real data. The data obtained here is a list, where each element is a tuple of (img, label)</li>
<li>a collate_fn function to combine the data into a mini-batch. So the data here is a list containing two elements, the tenser of img and label respectively.</li>
</ul>
<p>Thank you for reading this post. If you enjoyed it, please consider following me on Medium and <a href="https://twitter.com/Sue_sk79" target="_blank" rel="noopener noreferrer">twitter</a> for more content about productivity tools and AI! 🔥</p>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/3600/0*KGB2LDb3-ppXVajd" type="image/"/>
    </item>
    <item>
      <title>All you should know about translation equivariance/invariance in CNN</title>
      <link>https://suegk.github.io/posts/2023-03-25.html</link>
      <guid>https://suegk.github.io/posts/2023-03-25.html</guid>
      <source url="https://suegk.github.io/rss.xml">All you should know about translation equivariance/invariance in CNN</source>
      <description>All you should know about translation equivariance/invariance in CNN First published on Medium Translation invariance and translation equivariance are two important concepts in convolutional neural networks (CNNs) related to the network’s ability to recognise objects regardless of their position within an image.</description>
      <category>CV</category>
      <pubDate>Sat, 25 Mar 2023 00:00:00 GMT</pubDate>
      <content:encoded><![CDATA[<h2> All you should know about translation equivariance/invariance in CNN</h2>
<p>First published on <a href="https://medium.com/@sue.sk.guo/all-you-should-know-about-translation-equivariance-invariance-in-cnn-cbf2a2ad33cd" target="_blank" rel="noopener noreferrer">Medium</a></p>
<p>Translation invariance and translation equivariance are two important concepts in convolutional neural networks (CNNs) related to the network’s ability to recognise objects regardless of their position within an image.</p>
<figure><img src="https://cdn-images-1.medium.com/max/2394/1*hF9FN5Ruac76-s5zcfZ0tQ.png" alt="05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube</figcaption></figure>
<h2> Translation invariance</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2092/0*yddB6NGhDosJmw-q" alt="05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube</figcaption></figure>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*uZ8kByev1Lc5oiDh2jxr7Q.gif" alt="05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube</figcaption></figure>
<p><strong>Translation invariance</strong> means that a CNN is able to recognise an object in an image regardless of its location or translation within the image. In other words, the network’s output should remain the same even if the image is shifted or translated in any direction. This property is desirable because it allows the network to generalize well to different images of the same object with different translations.</p>
<p><strong>Summary:</strong></p>
<ul>
<li>Pooling layers help build shift invariance in convolutional networks.</li>
<li>Shift invariance means that the same maximum value will be found under the pooling kernel even if the image is shifted slightly.</li>
<li>However, this shift invariance is only locally true and may not hold if the image is shifted too much.</li>
<li>Pooling is not completely bulletproof with regards to shift invariance, but it can still identify the same features in an image regardless of their position.</li>
</ul>
<h2> Translation equivariance</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2128/0*emgASPn__BRxHofS" alt="05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube" tabindex="0" loading="lazy"><figcaption>05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube</figcaption></figure>
<p>Translation equivariance, on the other hand, means that the network’s output is related to the location of the object within the image. More specifically, if the input image is shifted or translated, the output of the network will also be shifted or translated accordingly. This property is useful for tasks such as object detection, where the location of the object within the image is important.</p>
<p>In CNNs, translation invariance is achieved through the use of pooling layers, which aggregate <strong>feature maps</strong> into a more compact representation while preserving the most important features. Meanwhile, translation equivariance is achieved through the use of convolutional layers, which apply a filter or kernel to the input image to <strong>extract local features</strong> that are then combined to form a larger, more complex feature map. By using a combination of convolutional and pooling layers, CNNs are able to achieve both translation invariance and equivariance, making them highly effective for image recognition tasks.</p>
<h2> Max Pooling breaks shift equivariance? — Try antialiaing</h2>
<figure><img src="https://cdn-images-1.medium.com/max/2000/0*Wyy2QtclkDCkUXbA.gif" alt="Making Convolutional Networks Shift-Invariant Again" tabindex="0" loading="lazy"><figcaption>Making Convolutional Networks Shift-Invariant Again</figcaption></figure>
<p>Convolutional neural networks (CNNs) are approximately shift equivalent through their convolutional layers. However, the use of Max Pooling layers can break shift equivariance (also known as translation equivariance). To address this issue, one solution is to use anti-aliasing techniques in Computer Vision, such as blurring the image and then down-sampling it. This technique can help preserve important features in the image while reducing the effect of minor shifts or translations, which in turn can improve the shift equivariance of the CNN.</p>
<p>You can check more in the <a href="https://richzhang.github.io/antialiased-cnns/" target="_blank" rel="noopener noreferrer">paper</a></p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*unim0oPZtXDmRnWSXz4cyQ.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<p>There is another research about <a href="http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html" target="_blank" rel="noopener noreferrer">Harmonic Networks: Deep Translation and Rotation Equivariance</a></p>
<figure><img src="https://cdn-images-1.medium.com/max/2000/1*wIjQ3UY25Tk6BqHiI_1jZg.gif" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> CNN and Poorly shift invariant</h2>
<p><a href="https://arxiv.org/abs/1805.12177" target="_blank" rel="noopener noreferrer">Why do deep convolutional networks generalize so poorly to small image transformations?</a></p>
<blockquote>
<p>Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network’s prediction.
In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that</p>
<ul>
<li>the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem,</li>
<li>and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set.
We discuss two possible solutions to this problem:
(1) antialiasing the intermediate representations and
(2) increasing data augmentation and show that they provide only a partial solution at best.
Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.</li>
</ul>
</blockquote>
<p>There are also some interesting opinions from (Chinese Platform)</p>
<p><a href="https://www.zhihu.com/question/301522740/answer/531606623" target="_blank" rel="noopener noreferrer">Since CNN has translation invariance to images, will it be effective to use image translation (shift) for data augmentation to train CNN?</a></p>
<ul>
<li>It is precisely because pooling itself has weak translation invariance and will lose some information that in tasks that require translation equivariance (such as detection and segmentation), convolutional layers with a stride of 2 are often used instead of pooling layers.</li>
<li>In many classification tasks, global pooling or pyramid pooling is often used at the end of the network to learn global features.</li>
<li>The translation invariance that can be used for classification mainly comes from the parameters. Because of the translation equivalence of convolutional layers, this kind of translation invariance is mainly learned by the final fully connected layer, and it is more difficult for networks without fully connected layers to have this property.</li>
<li>To summarize, the translation invariance of CNN mainly comes from data learning, and the structure can only bring very weak translation invariance, while learning relies on data augmentation.</li>
</ul>
<h3> Reference</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=a4Quhf9NhMY&amp;t=944s" target="_blank" rel="noopener noreferrer">05 Imperial’s Deep learning course: Equivariance and Invariance — YouTube</a></li>
<li><a href="https://chriswolfvision.medium.com/what-is-translation-equivariance-and-why-do-we-use-convolutions-to-get-it-6f18139d4c59" target="_blank" rel="noopener noreferrer">What is translation equivariance, and why do we use convolutions to get it? | by Christian Wolf | Medium</a></li>
<li><a href="https://richzhang.github.io/antialiased-cnns/" target="_blank" rel="noopener noreferrer">Making Convolutional Networks Shift-Invariant Again</a></li>
<li><a href="http://visual.cs.ucl.ac.uk/pubs/harmonicNets/index.html" target="_blank" rel="noopener noreferrer">Harmonic Networks: Deep Translation and Rotation Equivariance</a></li>
<li><a href="https://www.zhihu.com/question/301522740/answer/531606623" target="_blank" rel="noopener noreferrer">Since CNN has translation invariance to images, will it be effective to use image translation (shift) for data augmentation to train CNN?</a></li>
</ul>
]]></content:encoded>
      <enclosure url="https://cdn-images-1.medium.com/max/2394/1*hF9FN5Ruac76-s5zcfZ0tQ.png" type="image/png"/>
    </item>
    <item>
      <title>Tools</title>
      <link>https://suegk.github.io/Algorithm/Conference-Workshop.html</link>
      <guid>https://suegk.github.io/Algorithm/Conference-Workshop.html</guid>
      <source url="https://suegk.github.io/rss.xml">Tools</source>
      <description>Tools AMiner 🌟 AMiner (aminer.org) aims to provide comprehensive search and mining services for researcher social networks. In this system, we focus on: (1) creating a semantic-based profile for each researcher by extracting information from the distributed Web; (2) integrating academic data (e.g., the bibliographic data and the researcher profiles) from multiple sources; (3) accurately searching the heterogeneous network; (4) analyzing and discovering interesting patterns from the built researcher social network. The main search and analysis functions in AMiner include:</description>
      <pubDate>Mon, 27 Mar 2023 00:43:32 GMT</pubDate>
      <content:encoded><![CDATA[<h1> Tools</h1>
<h2> <a href="https://www.aminer.cn/introduction" target="_blank" rel="noopener noreferrer">AMiner</a> 🌟</h2>
<blockquote>
<p><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">AMiner  </a>(<a href="http://aminer.org" target="_blank" rel="noopener noreferrer">aminer.org</a>) aims to provide comprehensive search and mining services for researcher social networks. In this system, we focus on:</p>
<p>(1) creating a semantic-based profile for each researcher by extracting information from the distributed Web;</p>
<p>(2) integrating academic data (e.g., the bibliographic data and the researcher profiles) from multiple sources;</p>
<p>(3) accurately searching the heterogeneous network;</p>
<p>(4) analyzing and discovering interesting patterns from the built researcher social network. The main search and analysis functions in AMiner include:</p>
</blockquote>
<ul>
<li><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">Profile search</a>: input a researcher name (e.g.,<a href="http://aminer.org/profile/jie-tang/53f46a3edabfaee43ed05f08" target="_blank" rel="noopener noreferrer">Jie Tang</a>), the system will return the semantic-based profile created for the researcher using information extraction techniques. In the profile page, the extracted and integrated information include: contact information, photo, citation statistics, academic achievement evaluation, (temporal) research interest, educational history, personal social graph, research funding (currently only US and CN), and publication records (including citation information, and the papers are automatically assigned to several different domains).</li>
<li><a href="http://aminer.org/" target="_blank" rel="noopener noreferrer">Expert finding</a>: input a query (e.g., data mining), the system will return experts on this topic. In addition, the system will suggest the top conference and the top ranked papers on this topic. There are two ranking algorithms, VSM and ACT. The former is similar to the conventional language model and the latter is based on our Author-Conference-Topic (ACT) model. Users can also provide feedbacks to the search results.</li>
<li><a href="https://cn.aminer.org/ranks/conf" target="_blank" rel="noopener noreferrer">Conference analysis</a>: input a conference name (e.g., KDD), the system returns who are the most active researchers on this conference, and the top-ranked papers.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Course search</a>: input a query (e.g., data mining), the system will tell you who are teaching courses relevant to the query.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Sub-graph search</a>: input a query (e.g., data mining), the system first tells you what topics are relevant to the query (e.g., five topics "Data mining", "XML Data", "Data Mining / Query Processing", "Web Data / Database design", "Web Mining" are relevant), and then display the most important sub-graph discovered on each relevant topic, augmented with a summary for the sub-graph.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">Topic browser</a>: based on our Author-Conference-Topic (ACT) model, we automatically discover 200 hot topics from the publications. For each topic, we automatically assign a label to represent its meanings. Furthermore, the browser presents the most active researchers, the most relevant conferences/papers, and the evolution trend of the topic is discovered.</li>
<li><a href="https://cn.aminer.org/academicstatistics" target="_blank" rel="noopener noreferrer">Academic ranks</a>: we define 8  <a href="http://aminer.org/AcademicStatistics" target="_blank" rel="noopener noreferrer">measures  </a>to evaluate the researcher's achievement. The  <a href="http://aminer.org/AcademicStatistics" target="_blank" rel="noopener noreferrer">measures  </a>include "h -index", "Citation", "Uptrend, "Activity", "Longevity", "Diversity, "Sociability", "New Star". For each measure, we output a ranking list in different domains. For example, one can search who have the highest citation number in the "data mining" domain.</li>
<li><a href="https://www.aminer.cn/#" target="_blank" rel="noopener noreferrer">User management</a>: one can register as a user to: (1) modify the extracted profile information; (2) provide feedback on the search results; (3) follow researchers in AMiner; (4) create an AMiner page (which can be used to advertise confs/workshops, or recruit students).</li>
</ul>
<figure><img src="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/X64Hh3.png" alt="X64Hh3" tabindex="0" loading="lazy"><figcaption>X64Hh3</figcaption></figure>
<figure><img src="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/HXLtQX.png" alt="HXLtQX" tabindex="0" loading="lazy"><figcaption>HXLtQX</figcaption></figure>
<p>Keep you updated with state-of-the-art technology is really important in the Machine learning and deep learning field.</p>
<p>There are some tips from Andre NG on how to read research papers. <a href="https://www.youtube.com/watch?v=733m6qBH-jI" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=733m6qBH-jI</a></p>
<p>So, where can we find the popular papers?</p>
<h2> Paperswithcode — <a href="https://paperswithcode.com/sota" target="_blank" rel="noopener noreferrer">Browse State-of-the-Art</a></h2>
<p>The papers are well categorized so you can follow what Andre said, choose an area of interest like semantic segmentation, and read 15–20 papers to get a good understanding of this field. More importantly, you can find the paper’s code.</p>
<p><img src="https://miro.medium.com/max/1368/1*tGqJ4kOcwBtYH8qZf5oowQ.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1048/1*GFD5JPhEEXzp7vwMmKfpjw.png" alt="" loading="lazy"></p>
<h2> <a href="https://nn.labml.ai/index.html" target="_blank" rel="noopener noreferrer">labml.ai Deep Learning Paper Implementations</a></h2>
<p>59 Implementations/tutorials of deep learning papers with side-by-side notes 📝; including transformers (original, xl, switch, feedback, vit, …), optimizers (adam, adabelief, …), gans(cyclegan, stylegan2, …), 🎮 reinforcement learning (ppo, dqn), capsnet, distillation, … 🧠</p>
<p>This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations. So you are able to read the paper while understanding how to implement it by Pytorch.</p>
<figure><img src="https://miro.medium.com/max/1400/1*QI_S19lNihupZgpJj7eMfg.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="http://labml.ai" target="_blank" rel="noopener noreferrer">labml.ai</a> — <a href="https://papers.labml.ai/" target="_blank" rel="noopener noreferrer">Trending Research Papers</a></h2>
<p>The most popular research papers on social media like Twitter. You can easily find links to download papers, paper summaries, explanation videos, and discussions.</p>
<p><img src="https://miro.medium.com/max/1400/1*7pr_Bk_Sh38aNgmzOPgb1A.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*XJkaJCU4qvndNY1pUV5VZw.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*MxxeMhbirfw3CcN9Vs4eRg.png" alt="" loading="lazy"></p>
<p>The chrome extension is really helpful as well.</p>
<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>This extension shows you the following details about research papers:
✨ 2-line summary
✨ Availability source code, videos, and discussions
✨ Popularity on Twitter
✨ Conferences

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2> <a href="https://aman.ai/papers/#noise-contrastive-estimation-a-new-estimation-principle-for-unnormalized-statistical-models" target="_blank" rel="noopener noreferrer">Paper lists made by Aman</a></h2>
<p>A summary of key papers in Computer Vision, NLP, and Speech recognition.</p>
<figure><img src="https://miro.medium.com/max/1400/1*BdkxFnegXS0g0sHUNW-Akw.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>
<h2> <a href="https://deeplearn.org/" target="_blank" rel="noopener noreferrer">Deep Learning Monitor</a></h2>
<p>Another website where you can find the hot papers on social media.</p>
<p>The great feature is that you can create some monitors with the keywords related to the topic of interest and check new updates every one or two weeks. Once you find a good paper and you log in Mendeley on this website, you can directly send it to your Mendeley account.</p>
<p><img src="https://miro.medium.com/max/1400/1*3HtQpm1hSI6ApIAIOl3YvQ.png" alt="" loading="lazy"><img src="https://miro.medium.com/max/1400/1*cz1BAvWn42uztuayw0dm-w.png" alt="" loading="lazy"></p>
]]></content:encoded>
      <enclosure url="https://testksj.oss-cn-beijing.aliyuncs.com/uPic/X64Hh3.png" type="image/png"/>
    </item>
  </channel>
</rss>